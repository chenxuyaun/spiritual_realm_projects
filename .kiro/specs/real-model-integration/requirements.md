# 真实模型集成与验证 - 需求文档

## 概述
基于阶段一调研报告，扩展现有MuAI编排系统以支持真实LLM模型（Qwen-Chat、GPT-2）的集成、端到端功能验证和性能基准测试。

## 用户故事

### 1. 真实模型加载与管理

#### 1.1 HuggingFace模型加载
作为系统管理员，我希望能够加载HuggingFace格式的真实LLM模型（Qwen-Chat、GPT-2），以便系统能够执行真实的推理任务。

**验收标准：**
- AC1.1.1: 支持通过AutoModelForCausalLM加载Qwen-Chat模型，启用trust_remote_code=True
- AC1.1.2: 支持加载GPT-2系列模型（gpt2, gpt2-medium, gpt2-large）
- AC1.1.3: 模型加载失败时提供清晰的错误信息和回退策略
- AC1.1.4: 支持指定设备（CPU/GPU）和数据类型（fp32/fp16/bf16）

#### 1.2 模型量化支持
作为系统管理员，我希望能够加载量化后的模型，以便在资源受限环境下运行大模型。

**验收标准：**
- AC1.2.1: 支持8-bit量化加载（bitsandbytes）
- AC1.2.2: 支持4-bit量化加载（GPTQ/bitsandbytes）
- AC1.2.3: 量化模型的推理结果与全精度模型保持合理一致性
- AC1.2.4: 量化加载失败时自动回退到全精度模式

#### 1.3 模型缓存与资源管理
作为系统管理员，我希望系统能够智能管理多个模型的内存占用，以便在有限资源下运行多种模型。

**验收标准：**
- AC1.3.1: 实现LRU缓存策略，最多缓存3个模型
- AC1.3.2: 支持模型卸载以释放GPU/CPU内存
- AC1.3.3: 提供内存使用监控和告警机制
- AC1.3.4: 支持GPU/CPU自动切换（GPU不可用时回退到CPU）

### 2. 端到端功能验证

#### 2.1 SearchQA场景验证
作为开发者，我希望验证真实模型在SearchQA场景下的功能完整性，以确保检索增强问答流程正常工作。

**验收标准：**
- AC2.1.1: 模型能够正确解析包含检索资料的提示词
- AC2.1.2: 生成的答案能够引用提供的资料内容
- AC2.1.3: 当资料不足以回答时，模型能够承认不知道
- AC2.1.4: 支持多轮追问，保持上下文一致性

#### 2.2 LessonPack场景验证
作为开发者，我希望验证真实模型在LessonPack场景下的功能完整性，以确保教学内容生成流程正常工作。

**验收标准：**
- AC2.2.1: 模型能够生成结构化的课程大纲
- AC2.2.2: 支持多轮对话完善课程内容
- AC2.2.3: 生成内容符合Markdown格式要求
- AC2.2.4: 支持指令修改（如扩充内容、添加测验题）

#### 2.3 多轮对话支持
作为用户，我希望与模型进行多轮对话，以便逐步完善任务结果。

**验收标准：**
- AC2.3.1: Qwen-Chat支持通过history参数维护对话历史
- AC2.3.2: GPT-2通过提示词拼接模拟对话历史
- AC2.3.3: 对话历史超出上下文窗口时进行智能截断
- AC2.3.4: 多轮对话中保持角色和风格一致性

### 3. 性能基准测试

#### 3.1 推理延迟测试
作为开发者，我希望测量模型的推理延迟，以便评估系统响应性能。

**验收标准：**
- AC3.1.1: 测量首token延迟（Time to First Token, TTFT）
- AC3.1.2: 测量每token生成延迟（Token/s）
- AC3.1.3: 测量端到端响应延迟
- AC3.1.4: 支持不同输入长度和输出长度的延迟测试

#### 3.2 内存占用测试
作为开发者，我希望监控模型的内存占用，以便优化资源配置。

**验收标准：**
- AC3.2.1: 测量模型加载时的峰值内存占用
- AC3.2.2: 测量推理过程中的内存增长（KV缓存）
- AC3.2.3: 对比全精度与量化模型的内存占用差异
- AC3.2.4: 提供内存使用报告和可视化

#### 3.3 吞吐量测试
作为开发者，我希望测量系统的吞吐量，以便评估并发处理能力。

**验收标准：**
- AC3.3.1: 测量单请求吞吐量（tokens/s）
- AC3.3.2: 测量并发请求下的总吞吐量
- AC3.3.3: 对比不同批处理大小的吞吐量差异
- AC3.3.4: 提供吞吐量基准报告

### 4. 推理优化

#### 4.1 FlashAttention集成
作为开发者，我希望启用FlashAttention加速，以提升大模型推理性能。

**验收标准：**
- AC4.1.1: 检测并启用FlashAttention 2（如果可用）
- AC4.1.2: FlashAttention不可用时自动回退到标准注意力
- AC4.1.3: 验证FlashAttention启用后的性能提升
- AC4.1.4: 确保FlashAttention不影响推理结果正确性

#### 4.2 批处理推理
作为开发者，我希望支持批处理推理，以提升多请求场景下的吞吐量。

**验收标准：**
- AC4.2.1: 支持多个请求的批量推理
- AC4.2.2: 实现动态批处理（根据请求队列自动组批）
- AC4.2.3: 批处理不影响单个请求的结果正确性
- AC4.2.4: 提供批处理大小的配置选项

## 非功能性需求

### 性能要求
- NFR1: Qwen-7B-Chat在A100 GPU上生成速度≥30 tokens/s
- NFR2: GPT-2在CPU上生成速度≥5 tokens/s
- NFR3: 模型加载时间≤60秒（首次加载，不含下载）
- NFR4: 内存占用不超过可用资源的80%

### 可靠性要求
- NFR5: 模型推理失败率<0.1%
- NFR6: 系统支持优雅降级（大模型不可用时切换小模型）
- NFR7: 提供完整的错误日志和诊断信息

### 兼容性要求
- NFR8: 支持Python 3.8+
- NFR9: 支持PyTorch 2.0+
- NFR10: 支持transformers 4.32+
- NFR11: 支持CUDA 11.8+（GPU模式）

## 约束条件
- C1: 必须与现有ModelManager接口兼容
- C2: 必须支持现有的工作流架构
- C3: 配置通过YAML文件管理
- C4: 测试覆盖率≥80%
