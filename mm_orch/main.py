"""
MuAIå¤šæ¨¡å‹ç¼–æ’ç³»ç»Ÿ - å‘½ä»¤è¡Œæ¥å£

æä¾›äº¤äº’å¼å¯¹è¯æ¨¡å¼å’Œå•æ¬¡æŸ¥è¯¢æ¨¡å¼çš„å‘½ä»¤è¡Œå·¥å…·ã€‚

éœ€æ±‚: 1.1
æ”¯æŒçœŸå®æ¨¡å‹é›†æˆå’ŒåŸºå‡†æµ‹è¯•ã€‚
"""

import argparse
import sys
import uuid
import json
from typing import Optional, Dict, Any

from mm_orch.schemas import UserRequest, WorkflowType
from mm_orch.orchestrator import get_orchestrator, create_orchestrator
from mm_orch.router import get_router
from mm_orch.consciousness.core import get_consciousness
from mm_orch.logger import get_logger, configure_logger

# Phase B integration with fallback
try:
    from mm_orch.orchestration.phase_b_orchestrator import get_phase_b_orchestrator

    PHASE_B_AVAILABLE = True
except ImportError:
    PHASE_B_AVAILABLE = False


logger = get_logger(__name__)


class CLI:
    """
    å‘½ä»¤è¡Œæ¥å£ç±»

    æ”¯æŒ:
    - å•æ¬¡æŸ¥è¯¢æ¨¡å¼: æ‰§è¡Œå•ä¸ªæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
    - äº¤äº’å¼å¯¹è¯æ¨¡å¼: æŒç»­å¯¹è¯ç›´åˆ°ç”¨æˆ·é€€å‡º
    - æŒ‡å®šå·¥ä½œæµæ¨¡å¼: ç›´æ¥æŒ‡å®šè¦ä½¿ç”¨çš„å·¥ä½œæµ
    - æ¨¡å‹é€‰æ‹©: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹
    - åŸºå‡†æµ‹è¯•: è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    """

    def __init__(
        self,
        orchestrator=None,
        verbose: bool = False,
        model: Optional[str] = None,
        use_real_models: bool = False,
        use_phase_b: bool = False,
    ):
        """
        åˆå§‹åŒ–CLI

        Args:
            orchestrator: å·¥ä½œæµç¼–æ’å™¨å®ä¾‹
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
            model: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç§°
            use_real_models: æ˜¯å¦ä½¿ç”¨çœŸå®æ¨¡å‹
            use_phase_b: æ˜¯å¦ä½¿ç”¨Phase B orchestrator (with fallback to Phase A)
        """
        # Use Phase B orchestrator if requested and available
        if use_phase_b and PHASE_B_AVAILABLE:
            logger.info("Using Phase B orchestrator with Phase A fallback")
            self.orchestrator = orchestrator or get_phase_b_orchestrator()
            self.using_phase_b = True
        else:
            if use_phase_b and not PHASE_B_AVAILABLE:
                logger.warning("Phase B requested but not available, using Phase A")
            self.orchestrator = orchestrator or get_orchestrator()
            self.using_phase_b = False

        self.verbose = verbose
        self.model = model
        self.use_real_models = use_real_models
        self.session_id: Optional[str] = None

    def run_single_query(self, query: str, workflow: Optional[str] = None) -> str:
        """
        æ‰§è¡Œå•æ¬¡æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            workflow: å¯é€‰çš„å·¥ä½œæµç±»å‹

        Returns:
            str: æŸ¥è¯¢ç»“æœ
        """
        try:
            if workflow:
                # æŒ‡å®šå·¥ä½œæµæ¨¡å¼
                workflow_type = self._parse_workflow_type(workflow)
                if workflow_type is None:
                    return f"é”™è¯¯: æœªçŸ¥çš„å·¥ä½œæµç±»å‹ '{workflow}'"

                result = self.orchestrator.execute_workflow(
                    workflow_type=workflow_type,
                    parameters={"query": query, "topic": query, "message": query},
                )
            else:
                # è‡ªåŠ¨è·¯ç”±æ¨¡å¼
                request = UserRequest(query=query)
                result = self.orchestrator.process_request(request)

            if result.status == "success":
                return self._format_result(result.result)
            elif result.status == "partial":
                output = self._format_result(result.result)
                if result.error:
                    output += f"\n\n[è­¦å‘Š: {result.error}]"
                return output
            else:
                return f"é”™è¯¯: {result.error or 'æœªçŸ¥é”™è¯¯'}"

        except Exception as e:
            logger.error("Query execution failed", error=str(e))
            return f"æ‰§è¡Œé”™è¯¯: {str(e)}"

    def run_interactive(self) -> None:
        """
        è¿è¡Œäº¤äº’å¼å¯¹è¯æ¨¡å¼
        """
        self.session_id = str(uuid.uuid4())

        print("\n" + "=" * 60)
        print("MuAIå¤šæ¨¡å‹ç¼–æ’ç³»ç»Ÿ - äº¤äº’å¼å¯¹è¯æ¨¡å¼")
        print("=" * 60)
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        print("è¾“å…¥ 'status' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("è¾“å…¥ 'workflow <type>' åˆ‡æ¢å·¥ä½œæµæ¨¡å¼")
        print("=" * 60 + "\n")

        current_workflow: Optional[str] = None

        while True:
            try:
                # æ˜¾ç¤ºæç¤ºç¬¦
                prompt = f"[{current_workflow or 'auto'}] > " if current_workflow else "> "
                user_input = input(prompt).strip()

                if not user_input:
                    continue

                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                lower_input = user_input.lower()

                if lower_input in ("quit", "exit", "q"):
                    print("\nå†è§ï¼")
                    break

                if lower_input == "help":
                    self._print_help()
                    continue

                if lower_input == "status":
                    self._print_status()
                    continue

                if lower_input.startswith("workflow "):
                    workflow_name = user_input[9:].strip()
                    if workflow_name == "auto":
                        current_workflow = None
                        print("å·²åˆ‡æ¢åˆ°è‡ªåŠ¨è·¯ç”±æ¨¡å¼")
                    elif self._parse_workflow_type(workflow_name):
                        current_workflow = workflow_name
                        print(f"å·²åˆ‡æ¢åˆ° {workflow_name} å·¥ä½œæµ")
                    else:
                        print(f"æœªçŸ¥çš„å·¥ä½œæµç±»å‹: {workflow_name}")
                        self._print_available_workflows()
                    continue

                if lower_input == "workflows":
                    self._print_available_workflows()
                    continue

                if lower_input == "clear":
                    self.session_id = str(uuid.uuid4())
                    print("å·²æ¸…é™¤å¯¹è¯å†å²ï¼Œå¼€å§‹æ–°ä¼šè¯")
                    continue

                # æ‰§è¡ŒæŸ¥è¯¢
                print()  # ç©ºè¡Œ
                result = self._execute_interactive_query(user_input, current_workflow)
                print(result)
                print()  # ç©ºè¡Œ

            except KeyboardInterrupt:
                print("\n\nå·²ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except EOFError:
                print("\nå†è§ï¼")
                break

    def _execute_interactive_query(self, query: str, workflow: Optional[str] = None) -> str:
        """
        æ‰§è¡Œäº¤äº’å¼æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            workflow: å¯é€‰çš„å·¥ä½œæµç±»å‹

        Returns:
            str: æŸ¥è¯¢ç»“æœ
        """
        try:
            if workflow:
                workflow_type = self._parse_workflow_type(workflow)
                if workflow_type is None:
                    return f"é”™è¯¯: æœªçŸ¥çš„å·¥ä½œæµç±»å‹ '{workflow}'"

                params = {
                    "query": query,
                    "topic": query,
                    "message": query,
                    "session_id": self.session_id,
                }
                result = self.orchestrator.execute_workflow(
                    workflow_type=workflow_type, parameters=params
                )
            else:
                request = UserRequest(query=query, session_id=self.session_id)
                result = self.orchestrator.process_request(request)

            # æ ¼å¼åŒ–è¾“å‡º
            output_parts = []

            if self.verbose and result.metadata:
                routing_info = result.metadata.get("routing", {})
                if routing_info:
                    output_parts.append(
                        f"[è·¯ç”±: {routing_info.get('workflow_type', 'unknown')}, "
                        f"ç½®ä¿¡åº¦: {routing_info.get('confidence', 0):.2f}]"
                    )

                exec_time = result.metadata.get("execution_time")
                if exec_time:
                    output_parts.append(f"[è€—æ—¶: {exec_time:.2f}s]")

            if result.status == "success":
                output_parts.append(self._format_result(result.result))
            elif result.status == "partial":
                output_parts.append(self._format_result(result.result))
                if result.error:
                    output_parts.append(f"\n[è­¦å‘Š: {result.error}]")
            else:
                output_parts.append(f"é”™è¯¯: {result.error or 'æœªçŸ¥é”™è¯¯'}")

            return "\n".join(output_parts)

        except Exception as e:
            logger.error("Interactive query failed", error=str(e))
            return f"æ‰§è¡Œé”™è¯¯: {str(e)}"

    def _format_result(self, result) -> str:
        """
        æ ¼å¼åŒ–ç»“æœè¾“å‡º

        Args:
            result: å·¥ä½œæµç»“æœ

        Returns:
            str: æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        if result is None:
            return "[æ— ç»“æœ]"

        if isinstance(result, str):
            return result

        if isinstance(result, dict):
            # å¤„ç†ç»“æ„åŒ–æ•™å­¦åŒ…ç»“æœ
            if "lesson_explain_structured" in result:
                return self._format_structured_lesson(result["lesson_explain_structured"])

            # å¤„ç†ä¼ ç»Ÿæ•™å­¦åŒ…ç»“æœ
            if "plan" in result and "explanation" in result:
                parts = []
                parts.append("=== æ•™å­¦è®¡åˆ’ ===")
                parts.append(result.get("plan", ""))
                parts.append("\n=== è®²è§£å†…å®¹ ===")
                parts.append(result.get("explanation", ""))

                exercises = result.get("exercises", [])
                if exercises:
                    parts.append("\n=== ç»ƒä¹ é¢˜ ===")
                    for i, ex in enumerate(exercises, 1):
                        parts.append(f"\né¢˜ç›® {i}: {ex.get('question', '')}")
                        parts.append(f"ç­”æ¡ˆ: {ex.get('answer', '')}")

                return "\n".join(parts)

            # å¤„ç†RAGç»“æœ
            if "answer" in result and "sources" in result:
                parts = [result.get("answer", "")]
                sources = result.get("sources", [])
                if sources:
                    parts.append("\n--- æ¥æº ---")
                    for src in sources[:3]:
                        parts.append(f"- {src}")
                return "\n".join(parts)

            # é€šç”¨å­—å…¸æ ¼å¼åŒ–
            return str(result)

        return str(result)

    def _format_structured_lesson(self, lesson_data: Dict[str, Any]) -> str:
        """
        Format structured lesson for CLI display.

        Args:
            lesson_data: StructuredLesson JSON dictionary

        Returns:
            Formatted string with clear sections, numbered examples, and bullet points

        Requirements:
            - 20.3: List examples with numbering
            - 20.4: Display key points as bullet points
        """
        from mm_orch.workflows.lesson_structure import StructuredLesson

        try:
            lesson = StructuredLesson.from_json(lesson_data)

            parts = []

            # Display topic and grade prominently
            parts.append("=" * 60)
            parts.append(f"ä¸»é¢˜: {lesson.topic}")
            parts.append(f"å¹´çº§/éš¾åº¦: {lesson.grade}")
            parts.append("=" * 60)
            parts.append("")

            # Display each section with clear headers
            for i, section in enumerate(lesson.sections, 1):
                parts.append(f"ã€ç¬¬{i}éƒ¨åˆ†ï¼š{section.name}ã€‘")
                parts.append("-" * 60)
                parts.append("")

                # Teacher content
                parts.append("æ•™å¸ˆè®²è§£:")
                parts.append(section.teacher_say)
                parts.append("")

                # Student responses (if present)
                if section.student_may_say:
                    parts.append("å­¦ç”Ÿå¯èƒ½çš„å›ç­”:")
                    parts.append(section.student_may_say)
                    parts.append("")

                # Examples with numbering
                if section.examples:
                    parts.append("ç¤ºä¾‹:")
                    for j, example in enumerate(section.examples, 1):
                        parts.append(f"  {j}. {example}")
                    parts.append("")

                # Questions with numbering
                if section.questions:
                    parts.append("é—®é¢˜:")
                    for j, question in enumerate(section.questions, 1):
                        parts.append(f"  {j}. {question}")
                    parts.append("")

                # Key points as bullet points
                if section.key_points:
                    parts.append("è¦ç‚¹:")
                    for point in section.key_points:
                        parts.append(f"  â€¢ {point}")
                    parts.append("")

                # Teaching tips (if present)
                if section.tips:
                    parts.append("æ•™å­¦æç¤º:")
                    parts.append(f"  ğŸ’¡ {section.tips}")
                    parts.append("")

            parts.append("=" * 60)

            return "\n".join(parts)

        except Exception as e:
            logger.error(f"Failed to format structured lesson: {e}")
            # Fallback to simple dict display
            return str(lesson_data)

    def _parse_workflow_type(self, workflow_name: str) -> Optional[WorkflowType]:
        """
        è§£æå·¥ä½œæµç±»å‹åç§°

        Args:
            workflow_name: å·¥ä½œæµåç§°

        Returns:
            WorkflowTypeæˆ–None
        """
        name_lower = workflow_name.lower().strip()

        # æ”¯æŒå¤šç§å‘½åæ–¹å¼
        mapping = {
            "search_qa": WorkflowType.SEARCH_QA,
            "searchqa": WorkflowType.SEARCH_QA,
            "search": WorkflowType.SEARCH_QA,
            "lesson_pack": WorkflowType.LESSON_PACK,
            "lessonpack": WorkflowType.LESSON_PACK,
            "lesson": WorkflowType.LESSON_PACK,
            "teach": WorkflowType.LESSON_PACK,
            "chat_generate": WorkflowType.CHAT_GENERATE,
            "chatgenerate": WorkflowType.CHAT_GENERATE,
            "chat": WorkflowType.CHAT_GENERATE,
            "rag_qa": WorkflowType.RAG_QA,
            "ragqa": WorkflowType.RAG_QA,
            "rag": WorkflowType.RAG_QA,
            "self_ask_search_qa": WorkflowType.SELF_ASK_SEARCH_QA,
            "selfasksearchqa": WorkflowType.SELF_ASK_SEARCH_QA,
            "self_ask": WorkflowType.SELF_ASK_SEARCH_QA,
            "selfask": WorkflowType.SELF_ASK_SEARCH_QA,
        }

        return mapping.get(name_lower)

    def _print_help(self) -> None:
        """æ‰“å°å¸®åŠ©ä¿¡æ¯"""
        help_text = """
å‘½ä»¤å¸®åŠ©:
  quit, exit, q    - é€€å‡ºç¨‹åº
  help             - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  status           - æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
  workflows        - æ˜¾ç¤ºå¯ç”¨çš„å·¥ä½œæµç±»å‹
  workflow <type>  - åˆ‡æ¢åˆ°æŒ‡å®šå·¥ä½œæµæ¨¡å¼
  workflow auto    - åˆ‡æ¢å›è‡ªåŠ¨è·¯ç”±æ¨¡å¼
  clear            - æ¸…é™¤å¯¹è¯å†å²ï¼Œå¼€å§‹æ–°ä¼šè¯

å·¥ä½œæµç±»å‹:
  search_qa        - æœç´¢é—®ç­”ï¼ˆç½‘ç»œæœç´¢ï¼‰
  lesson_pack      - æ•™å­¦å†…å®¹ç”Ÿæˆ
  chat_generate    - å¤šè½®å¯¹è¯
  rag_qa           - çŸ¥è¯†åº“é—®ç­”
  self_ask_search_qa - å¤æ‚é—®é¢˜åˆ†è§£æœç´¢

ç¤ºä¾‹:
  > ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ
  > workflow lesson
  > æ•™æˆ‘PythonåŸºç¡€
  > workflow auto
"""
        print(help_text)

    def _print_status(self) -> None:
        """æ‰“å°ç³»ç»ŸçŠ¶æ€"""
        stats = self.orchestrator.get_statistics()

        print("\n=== ç³»ç»ŸçŠ¶æ€ ===")
        print(f"ä¼šè¯ID: {self.session_id or 'æ— '}")
        print(f"å·²æ³¨å†Œå·¥ä½œæµ: {stats.get('registered_workflows', 0)}")
        print(f"æ‰§è¡Œæ¬¡æ•°: {stats.get('execution_count', 0)}")
        print(f"æˆåŠŸç‡: {stats.get('success_rate', 0):.1%}")
        print(f"å¹³å‡è€—æ—¶: {stats.get('average_execution_time', 0):.2f}s")

        # æ„è¯†çŠ¶æ€
        try:
            consciousness = get_consciousness()
            status = consciousness.get_status_summary()
            print(f"\n=== æ„è¯†çŠ¶æ€ ===")
            print(f"å‘å±•é˜¶æ®µ: {status.get('development_stage', 'unknown')}")
            emotion = status.get("emotion_state", {})
            print(
                f"æƒ…æ„ŸçŠ¶æ€: valence={emotion.get('valence', 0):.2f}, arousal={emotion.get('arousal', 0):.2f}"
            )
        except Exception:
            pass

        print()

    def _print_available_workflows(self) -> None:
        """æ‰“å°å¯ç”¨çš„å·¥ä½œæµ"""
        print("\nå¯ç”¨çš„å·¥ä½œæµç±»å‹:")
        print("  search_qa (search)     - æœç´¢é—®ç­”")
        print("  lesson_pack (lesson)   - æ•™å­¦å†…å®¹ç”Ÿæˆ")
        print("  chat_generate (chat)   - å¤šè½®å¯¹è¯")
        print("  rag_qa (rag)           - çŸ¥è¯†åº“é—®ç­”")
        print("  self_ask_search_qa     - å¤æ‚é—®é¢˜åˆ†è§£")
        print("  auto                   - è‡ªåŠ¨è·¯ç”±ï¼ˆé»˜è®¤ï¼‰")
        print()


def run_benchmark(
    model_name: str = "gpt2", output_dir: str = "data/benchmarks", output_format: str = "json"
) -> int:
    """
    è¿è¡ŒåŸºå‡†æµ‹è¯•

    Args:
        model_name: è¦æµ‹è¯•çš„æ¨¡å‹åç§°
        output_dir: è¾“å‡ºç›®å½•
        output_format: è¾“å‡ºæ ¼å¼ (json/csv)

    Returns:
        int: é€€å‡ºç 
    """
    try:
        from mm_orch.benchmark.latency import LatencyBenchmark
        from mm_orch.benchmark.memory import MemoryBenchmark
        from mm_orch.benchmark.throughput import ThroughputBenchmark
        from mm_orch.benchmark.reporter import BenchmarkReporter

        print(f"\n=== åŸºå‡†æµ‹è¯•: {model_name} ===\n")

        # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
        latency_bench = LatencyBenchmark()
        memory_bench = MemoryBenchmark()
        throughput_bench = ThroughputBenchmark()
        reporter = BenchmarkReporter(output_dir=output_dir)

        results = []

        # è¿è¡Œå»¶è¿Ÿæµ‹è¯•
        print("è¿è¡Œå»¶è¿Ÿæµ‹è¯•...")
        try:
            latency_result = latency_bench.run_benchmark(
                model_name=model_name,
                test_prompts=["Hello, how are you?", "What is Python?"],
                num_runs=3,
            )
            results.append(latency_result)
            print(f"  TTFT: {latency_result.metrics.get('avg_ttft', 0):.3f}s")
            print(f"  Tokens/s: {latency_result.metrics.get('avg_tokens_per_second', 0):.1f}")
        except Exception as e:
            print(f"  å»¶è¿Ÿæµ‹è¯•å¤±è´¥: {e}")

        # è¿è¡Œå†…å­˜æµ‹è¯•
        print("\nè¿è¡Œå†…å­˜æµ‹è¯•...")
        try:
            memory_result = memory_bench.run_benchmark(model_name=model_name)
            results.append(memory_result)
            print(f"  æ¨¡å‹å†…å­˜: {memory_result.metrics.get('model_memory_mb', 0):.1f} MB")
            print(f"  å³°å€¼å†…å­˜: {memory_result.metrics.get('peak_memory_mb', 0):.1f} MB")
        except Exception as e:
            print(f"  å†…å­˜æµ‹è¯•å¤±è´¥: {e}")

        # è¿è¡Œååé‡æµ‹è¯•
        print("\nè¿è¡Œååé‡æµ‹è¯•...")
        try:
            throughput_result = throughput_bench.run_benchmark(
                model_name=model_name, num_requests=5, concurrent_levels=[1, 2]
            )
            results.append(throughput_result)
            print(
                f"  å•è¯·æ±‚åå: {throughput_result.metrics.get('single_throughput', 0):.1f} tokens/s"
            )
        except Exception as e:
            print(f"  ååé‡æµ‹è¯•å¤±è´¥: {e}")

        # ç”ŸæˆæŠ¥å‘Š
        if results:
            print("\nç”ŸæˆæŠ¥å‘Š...")
            if output_format == "json":
                report_path = reporter.generate_json_report(results)
            else:
                report_path = reporter.generate_csv_report(results)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

        print("\n=== åŸºå‡†æµ‹è¯•å®Œæˆ ===\n")
        return 0

    except ImportError as e:
        print(f"é”™è¯¯: ç¼ºå°‘åŸºå‡†æµ‹è¯•æ¨¡å— - {e}")
        return 1
    except Exception as e:
        print(f"åŸºå‡†æµ‹è¯•é”™è¯¯: {e}")
        return 1


def show_model_info(model_name: Optional[str] = None) -> int:
    """
    æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯

    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼‰

    Returns:
        int: é€€å‡ºç 
    """
    try:
        import yaml
        from pathlib import Path

        # è¯»å–æ¨¡å‹é…ç½®
        config_path = Path("config/models.yaml")
        if not config_path.exists():
            print("é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶ config/models.yaml")
            return 1

        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        models = config.get("models", {})

        if model_name:
            # æ˜¾ç¤ºç‰¹å®šæ¨¡å‹ä¿¡æ¯
            if model_name not in models:
                print(f"é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹ '{model_name}'")
                print(f"å¯ç”¨æ¨¡å‹: {', '.join(models.keys())}")
                return 1

            model_config = models[model_name]
            print(f"\n=== æ¨¡å‹ä¿¡æ¯: {model_name} ===\n")
            print(f"  HuggingFaceåç§°: {model_config.get('model_name', 'N/A')}")
            print(f"  æ¨¡å‹ç±»å‹: {model_config.get('model_type', 'N/A')}")
            print(f"  è®¾å¤‡: {model_config.get('device', 'auto')}")
            print(f"  æ•°æ®ç±»å‹: {model_config.get('dtype', 'auto')}")
            print(f"  é‡åŒ–: {model_config.get('quantization', 'æ— ')}")
            print(f"  FlashAttention: {model_config.get('flash_attention', False)}")
            print(f"  æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {model_config.get('max_context_length', 'N/A')}")
            print()
        else:
            # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
            print("\n=== å¯ç”¨æ¨¡å‹ ===\n")
            for name, cfg in models.items():
                quant = cfg.get("quantization", "")
                quant_str = f" ({quant})" if quant else ""
                print(f"  {name}: {cfg.get('model_type', 'unknown')}{quant_str}")
            print()
            print("ä½¿ç”¨ --model-info <model_name> æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
            print()

        return 0

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return 1


def create_parser() -> argparse.ArgumentParser:
    """
    åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨

    Returns:
        argparse.ArgumentParser: å‚æ•°è§£æå™¨
    """
    parser = argparse.ArgumentParser(
        prog="mm_orch",
        description="MuAIå¤šæ¨¡å‹ç¼–æ’ç³»ç»Ÿ - å‘½ä»¤è¡Œæ¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•æ¬¡æŸ¥è¯¢ï¼ˆè‡ªåŠ¨è·¯ç”±ï¼‰
  python -m mm_orch "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
  
  # æŒ‡å®šå·¥ä½œæµ
  python -m mm_orch --workflow search_qa "æœ€æ–°çš„AIæ–°é—»"
  python -m mm_orch -w lesson "PythonåŸºç¡€æ•™ç¨‹"
  
  # ä½¿ç”¨çœŸå®æ¨¡å‹
  python -m mm_orch --real-models --model qwen-7b-chat "ä½ å¥½"
  
  # äº¤äº’å¼å¯¹è¯æ¨¡å¼
  python -m mm_orch --mode chat
  python -m mm_orch -m chat
  
  # è¿è¡ŒåŸºå‡†æµ‹è¯•
  python -m mm_orch --benchmark --model gpt2
  
  # æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
  python -m mm_orch --model-info
  python -m mm_orch --model-info gpt2
  
  # è¯¦ç»†è¾“å‡º
  python -m mm_orch -v "ä½ å¥½"
""",
    )

    parser.add_argument("query", nargs="?", help="è¦æ‰§è¡Œçš„æŸ¥è¯¢ï¼ˆå¦‚æœä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼ï¼‰")

    parser.add_argument(
        "-m",
        "--mode",
        choices=["query", "chat"],
        default="query",
        help="è¿è¡Œæ¨¡å¼: query=å•æ¬¡æŸ¥è¯¢, chat=äº¤äº’å¼å¯¹è¯ (é»˜è®¤: query)",
    )

    parser.add_argument(
        "-w",
        "--workflow",
        choices=[
            "search_qa",
            "lesson_pack",
            "chat_generate",
            "rag_qa",
            "self_ask_search_qa",
            "auto",
        ],
        default=None,
        help="æŒ‡å®šå·¥ä½œæµç±»å‹ï¼ˆé»˜è®¤: è‡ªåŠ¨è·¯ç”±ï¼‰",
    )

    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆå¦‚ gpt2, qwen-7b-chatï¼‰",
    )

    parser.add_argument(
        "--real-models",
        action="store_true",
        help="ä½¿ç”¨çœŸå®æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆéœ€è¦GPUæˆ–è¶³å¤Ÿçš„å†…å­˜ï¼‰",
    )

    # Phase B integration
    parser.add_argument(
        "--phase-b",
        action="store_true",
        help="ä½¿ç”¨Phase B orchestrator (graph-based execution with fallback to Phase A)",
    )

    # åŸºå‡†æµ‹è¯•å‚æ•°
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•",
    )

    parser.add_argument(
        "--benchmark-output",
        type=str,
        default="data/benchmarks",
        help="åŸºå‡†æµ‹è¯•è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: data/benchmarksï¼‰",
    )

    parser.add_argument(
        "--benchmark-format",
        choices=["json", "csv"],
        default="json",
        help="åŸºå‡†æµ‹è¯•æŠ¥å‘Šæ ¼å¼ï¼ˆé»˜è®¤: jsonï¼‰",
    )

    # æ¨¡å‹ä¿¡æ¯
    parser.add_argument(
        "--model-info",
        nargs="?",
        const="",
        default=None,
        help="æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ï¼ˆä¸å¸¦å‚æ•°æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹ï¼Œå¸¦å‚æ•°æ˜¾ç¤ºç‰¹å®šæ¨¡å‹ï¼‰",
    )

    parser.add_argument(
        "-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºï¼ˆåŒ…æ‹¬è·¯ç”±ä¿¡æ¯å’Œæ‰§è¡Œæ—¶é—´ï¼‰"
    )

    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="WARNING",
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: WARNING)",
    )

    parser.add_argument("--version", action="version", version="%(prog)s 1.0.0")

    return parser


def main(args=None) -> int:
    """
    ä¸»å…¥å£å‡½æ•°

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰

    Returns:
        int: é€€å‡ºç 
    """
    parser = create_parser()
    parsed_args = parser.parse_args(args)

    # é…ç½®æ—¥å¿—
    configure_logger(level=parsed_args.log_level)

    try:
        # å¤„ç†æ¨¡å‹ä¿¡æ¯å‘½ä»¤
        if parsed_args.model_info is not None:
            model_name = parsed_args.model_info if parsed_args.model_info else None
            return show_model_info(model_name)

        # å¤„ç†åŸºå‡†æµ‹è¯•å‘½ä»¤
        if parsed_args.benchmark:
            model_name = parsed_args.model or "gpt2"
            return run_benchmark(
                model_name=model_name,
                output_dir=parsed_args.benchmark_output,
                output_format=parsed_args.benchmark_format,
            )

        # åˆ›å»ºCLIå®ä¾‹
        cli = CLI(
            verbose=parsed_args.verbose,
            model=parsed_args.model,
            use_real_models=parsed_args.real_models,
            use_phase_b=parsed_args.phase_b,
        )

        # ç¡®å®šè¿è¡Œæ¨¡å¼
        if parsed_args.mode == "chat" or (not parsed_args.query and parsed_args.mode != "query"):
            # äº¤äº’å¼æ¨¡å¼
            cli.run_interactive()
            return 0

        if parsed_args.query:
            # å•æ¬¡æŸ¥è¯¢æ¨¡å¼
            workflow = parsed_args.workflow if parsed_args.workflow != "auto" else None
            result = cli.run_single_query(parsed_args.query, workflow)
            print(result)
            return 0

        # æ²¡æœ‰æŸ¥è¯¢ä¹Ÿæ²¡æœ‰æŒ‡å®šchatæ¨¡å¼ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼
        cli.run_interactive()
        return 0

    except KeyboardInterrupt:
        print("\nå·²ä¸­æ–­")
        return 130
    except Exception as e:
        logger.error("CLI error", error=str(e))
        print(f"é”™è¯¯: {str(e)}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())
