下一阶段开发说明（从 A 升级到“可扩展编排 + 可训练调度器”）

### 总目标

把现有的固定流水线（搜索→摘要→回答）重构为一个“可插拔的多工具/多模型编排框架”，支持多个工作流（search_qa、rag_qa、summarize_url、write_story、teach_explain…），并加入：

1. **工作流图（Graph/State）执行器**
2. **模型/工具注册表（Registry）**
3. **可观测性（Trace/Log/Cost）**
4. **评测与回归（Eval harness）**
5. **调度器训练数据采集 & 首版可训练 Router**

> 设计参考：HuggingGPT 把 LLM 当“控制器”，做任务规划→模型选择→执行→汇总的流水线。([arXiv][1])

---

# Phase B1：架构重构（1~2天内必须完成）

## 1) 统一“步骤”接口（Step API）

定义一个标准 Step 抽象（类或协议均可）：

* `name`
* `input_keys / output_keys`
* `run(state, runtime) -> state`

并把现有的步骤拆成独立 Step：

* `WebSearchStep`（用 ddgs）
* `FetchUrlStep`
* `SummarizeStep`
* `AnswerGenerateStep`

> 注意：duckduckgo_search 已明确改名为 ddgs，请确保依赖与 import 统一。([PyPI][2])

## 2) 引入“状态 State”（TypedDict / dataclass）

新增 `State`（推荐 TypedDict）作为贯穿全流程的数据载体：

* question
* search_results[]
* docs{url:text}
* summaries{url:summary}
* final_answer
* citations[]

参考 LangGraph 的“State 驱动图执行”理念：State 可以是 TypedDict / Pydantic / dataclass。([LangGraph][3])

## 3) 增加最小图执行器（Graph Runner）

实现一个最小 Graph Runner（不依赖第三方框架也可）：

* 支持线性链（先把线跑稳）
* 支持分支（后面扩展）
* 每个 Step 运行前/后都写 trace（见后面可观测性）

验收：

* 用 Graph 表达 A 链路并跑通
* 新增一个小链路不需要改 runner，只需注册步骤并写 workflow 定义

---

# Phase B2：模型/工具注册表 + 成本感知运行（关键）

## 4) Registry：模型、工具、工作流都注册化

实现 3 个注册表：

1. `ToolRegistry`：search/fetch（以及未来的计算器、翻译等）
2. `ModelRegistry`：summarizer/generator/qa/embedding…
3. `WorkflowRegistry`：workflow_name → graph(steps)

每个模型项必须带 metadata：

* `capabilities`（标签：summarize / generate / qa / story / teach…）
* `expected_vram_mb`
* `supports_quant`（是否支持 8bit/4bit）
* `preferred_device_policy`（gpu_on_demand / cpu_only / gpu_resident）

## 5) Runtime：显存与加载策略升级

在你已有的 ModelManager 基础上加 3 个能力：

* **模型使用计数**与“短暂驻留”策略（例如 30 秒内重复调用不卸载）
* **成本统计**：每步耗时、显存峰值、加载次数
* **量化开关**（为生成模型优先做 8bit/4bit）

  * Transformers 已官方支持 bitsandbytes 的 8bit/4bit 量化加载（以及 AWQ/GPTQ 等）。([Hugging Face][4])

> 后续你要“双模型协同”时，量化会非常关键；QLoRA 也证明 4bit 下仍能保持较好效果用于微调。([ar5iv][5])

验收：

* 输出一份 run trace：每步（load_time, step_time, vram_peak, unload_time）
* 生成模型可切换 8bit/4bit（至少 8bit 先成功）

---

# Phase B3：工作流扩展（马上加 3 条“新链路”）

在 WorkflowRegistry 中新增 3 个 workflow（只需复用 Step + 组合）：

1. `summarize_url`
   输入 URL → fetch → summarize → 输出摘要 + 引用

2. `search_qa_fast`
   search → fetch_top2 →（不做长摘要）→ generator 直接基于片段回答（更快、更省）

3. `search_qa_strict_citations`
   search → fetch_top3 → summarize_each → generator（要求每个要点后带引用编号）

验收：

* CLI/HTTP 入口能指定 workflow_name 强制执行
* 默认仍支持原来的 A（作为 search_qa_strict_citations 或 search_qa）

---

# Phase B4：可观测性与回归评测（为训练调度器做地基）

## 6) Trace & Dataset Logger（强制）

每次运行必须落盘一条 JSONL：

* request_id, question
* chosen_workflow
* steps trace（耗时、显存、异常）
* urls used
* final_answer
* 一个自动质量信号（见下条）

## 7) 自动评测信号（先粗糙也可以）

加入一些“可自动打分”的指标（先能用即可）：

* 是否调用了 search（对需要最新信息的问句）
* 引用数量 ≥ N
* 答案长度范围
* 是否包含“要点列表”
* 失败率、超时率

目的：给后面的调度器训练提供 **reward / label**。

---

# Phase C：调度器升级（从规则到可训练 Router）

> 参考 OpenAGI：提出 RLTF（从任务反馈改进规划/选择），我们这里先做一个工程可落地的简化版本。([arXiv][6])

## 8) Router v1：规则 + 置信度

实现 `route(question)->(workflow_name, confidence)`：

* 规则命中高 → high confidence
* 规则不明确 → default 到 `search_qa_fast` 或 `search_qa_strict_citations`

同时记录：router 的候选列表与理由（写入 trace）

## 9) Router v2：训练一个轻量分类器

用你 Phase B4 记录的 JSONL 生成训练集：

* 输入：question（可加简单特征：是否包含“最新/今天/上网/引用/写小说/解释”等）
* 标签：best_workflow（先用“规则选择”或“离线跑多条 workflow 选最好”生成弱标签）

模型建议：

* MiniLM / DistilBERT 做文本分类（快、省）

验收：

* 训练脚本：train_router.py 输出 router.pt
* 推理时：Router v2 的选择能替代 v1，且回归集上准确率提升或成本更低

## 10) 成本感知路由（下一小步）

在 Router 输出中加入 cost-aware：

* 目标：最大化质量 - λ * cost（cost=延迟+加载次数+显存峰值）
* 简化实现：同一个问题上，Router 预测两个候选 workflow，挑 cost 更低者（在质量差不多时）
