## 开发任务描述（请按顺序实现）

### 目标（MVP）

实现一个在 **NVIDIA T4 15GB** 上运行的多模型编排 Demo：
用户输入问题 → 系统自动 **web_search** → 抓取网页正文 → **summarize** → **generate** 最终回答，并输出引用来源 URL。
要求：模型**按需加载到 GPU**，用完卸载，避免显存长期占用；默认同一时间只允许 1 个模型在 GPU。

---

## 1) 环境与依赖

使用 Python + PyTorch + HuggingFace Transformers（可用 pipeline 快速落地）。Transformers 的 pipeline 用法参考官方文档。([Hugging Face][2])

**依赖建议：**

* torch, transformers, accelerate
* bitsandbytes（后续可选，用于 8bit/4bit 量化加载，降低显存）([Hugging Face][3])
* ddgs（DuckDuckGo 搜索库；duckduckgo-search 已改名 ddgs）([PyPI][4])
* requests
* trafilatura（或 readability-lxml / beautifulsoup4）用于网页正文抽取

---

## 2) 项目结构（先搭骨架）

创建项目目录 `mm_orch/`，实现以下文件（全部可先写最小版本）：

```
mm_orch/
  main.py
  router.py
  runner.py
  runtime/
    model_manager.py
  tools/
    web_search.py
    fetch_url.py
  models/
    summarizer.py
    generator.py
  schemas.py
```

---

## 3) 数据结构（schemas.py）

定义可序列化的数据结构（dataclass 或 pydantic 均可）：

* `SearchResult {title, url, snippet}`
* `PlanStep {name, args}`
* `Plan {steps: List[PlanStep]}`
* `RunContext {question, search_results, docs, summaries, final_answer, citations}`

---

## 4) 工具层：搜索与网页抓取

### 4.1 tools/web_search.py

实现函数：

* `search_web(query: str, k: int = 5) -> List[SearchResult]`

实现方式：

* 优先用 `ddgs` 做文本搜索（无需 API Key），返回 title/url/snippet
  （注意：PyPI 上说明 duckduckgo_search 已改名 ddgs）([PyPI][4])

**验收：**

* 输入 query，能返回至少 5 条结果（若少于 5 也可，但要有容错）
* URL 去重；过滤明显非 http(s)

### 4.2 tools/fetch_url.py

实现函数：

* `fetch_and_extract(url: str, timeout=10, max_chars=12000) -> str`

要求：

* requests 拉取 HTML
* 用 trafilatura/readability 提取正文
* 超长截断（max_chars）
* 失败返回空字符串，并记录日志原因（状态码/异常）

**验收：**

* 对 5 个常见新闻/百科链接能抽取出正文（不为空）

---

## 5) 模型层：摘要与生成（按需加载）

### 5.1 runtime/model_manager.py（核心）

实现 `ModelManager`，必须支持：

* 同一时间最多 1 个 pipeline 在 GPU（默认策略）
* `get_summarizer()` / `get_generator()`：懒加载到 GPU
* `unload_current()`：删除 pipeline/模型，`gc.collect()` + `torch.cuda.empty_cache()`

⚠️ 注意：Transformers pipeline 适合 MVP，后续要更精细显存控制可改成 AutoModel + tokenizer 手动 `.to("cuda")`。

### 5.2 models/summarizer.py

实现：

* `summarize_text(text: str) -> str`

建议：

* 直接用 `pipeline("summarization", model="t5-small" or 小型BART/T5)`
  （模型名先固定写死，后续再做注册表）

要求：

* 输入长文本，先分段（例如每段 2k-3k chars），逐段摘要后再合并二次摘要（可选）
* 输出 5~8 条要点（bullet）或 1 段摘要

### 5.3 models/generator.py

实现：

* `generate_answer(question: str, bullet_summaries: str, citations: List[str]) -> str`

建议：

* 用 `pipeline("text-generation", model="distilgpt2")`
* Prompt 模板要求：必须在回答末尾列出引用 URL（你可以做成 `【1】url`）

---

## 6) 路由与执行：A 流水线（search → summarize → answer）

### 6.1 router.py

先做“固定计划路由”（只支持 A）：

* `route(question: str) -> Plan`
  返回 steps：

1. `web_search(k=5)`
2. `fetch_urls(top_n=3)`
3. `summarize_each_doc()`
4. `compose_final_answer()`

> 这就是 HuggingGPT 的“任务规划→模型选择→执行→回复”最小子集实现。([arXiv][1])

### 6.2 runner.py

实现：

* `run_plan(plan: Plan, question: str) -> RunContext`

执行逻辑（必须按顺序）：

1. `search_web(question, k=5)`
2. 取 top3 url，循环抓取正文 `fetch_and_extract`
3. 对每篇正文调用 `summarize_text` 得到摘要（每次摘要前，确保 summarizer 在 GPU；用完可先卸载，或等步骤结束一次卸载）
4. 汇总所有摘要为 bullet list
5. 调用 generator 输出最终回答，并附带引用列表

**强制显存策略（验收点）：**

* summarize 步骤结束后：卸载 summarizer
* generate 步骤结束后：卸载 generator
* 运行前后打印一次 `torch.cuda.memory_allocated()` / `reserved()` 作为日志

---

## 7) main.py：命令行 Demo

实现可运行入口：

* `python -m mm_orch.main "你的问题..."`

输出结构：

* 搜索结果（标题+URL）
* 抓取到的正文长度
* 每篇摘要
* 最终回答（含引用）

---

## 8) 可选增强（第二天就能加上）

### 8.1 量化降低显存（推荐尽早做）

为生成模型增加 `--quant 8bit|4bit` 选项：
使用 bitsandbytes 的 Transformers 集成（官方量化文档）([Hugging Face][3])
这样你以后更容易在 T4 上并行“一个小BERT类 + 一个生成模型”。

### 8.2 用“图/状态机”表达流水线（为未来多分支做准备）

把 runner 改成 state graph（后续可对接 LangGraph 思想：状态、序列、分支、循环）。([LangGraph][5])

---

## 9) 最小验收用例（请写成 tests 或 scripts）

* 问题：`"2026年最新的x（你自选一个热点）发生了什么？请给出3点总结并附来源"`
* 期待：

  * 发生了 `web_search`
  * 抓取了 ≥2 篇正文
  * 产生了摘要
  * 最终回答末尾有引用 URL（≥2 条）