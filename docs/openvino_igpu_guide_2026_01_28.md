# Intel iGPUåŠ é€ŸæŒ‡å—

**æ—¥æœŸ**: 2026-01-28  
**ç¡¬ä»¶**: Intel Core Ultra 5 125H + Intel Arc Graphics  
**çŠ¶æ€**: âœ… å·²éªŒè¯

---

## ç¡¬ä»¶é…ç½®

- **CPU**: Intel Core Ultra 5 125H (14æ ¸/18çº¿ç¨‹)
- **iGPU**: Intel Arc Graphics (é›†æˆæ˜¾å¡)
- **NPU**: Intel AI Boost (æ£€æµ‹åˆ°ä½†GPT-2ä¸æ”¯æŒ)
- **å†…å­˜**: 31.57 GB

---

## æ€§èƒ½åŸºå‡†

### CPU vs iGPUå¯¹æ¯”

| æŒ‡æ ‡ | CPU | iGPU | æå‡ |
|------|-----|------|------|
| å»¶è¿Ÿ | 522ms | 522ms | 1.0x |
| é€Ÿåº¦ | 57 tokens/s | 57 tokens/s | 1.0x |
| åŠ è½½æ—¶é—´ | 0.49s | 13s | 0.04x |

**æ³¨æ„**: å½“å‰æµ‹è¯•æ˜¾ç¤ºæ€§èƒ½ç›¸å½“ï¼Œä½†iGPUæœ‰ä¼˜åŒ–ç©ºé—´ã€‚

---

## ä½¿ç”¨OpenVINO iGPU

### 1. åŸºæœ¬ä½¿ç”¨

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹åˆ°iGPU
model = OVModelForCausalLM.from_pretrained(
    "models/openvino/gpt2",
    device="GPU",  # ä½¿ç”¨iGPU
    compile=True
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# ç”Ÿæˆæ–‡æœ¬
inputs = tokenizer("Hello, my name is", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
text = tokenizer.decode(outputs[0])
```

### 2. æ€§èƒ½ä¼˜åŒ–é…ç½®

```python
# ä¼˜åŒ–çš„iGPUé…ç½®
ov_config = {
    "PERFORMANCE_HINT": "LATENCY",  # ä¼˜åŒ–å»¶è¿Ÿ
    "NUM_STREAMS": "1",              # å•æµæ¨¡å¼
    "CACHE_DIR": "models/cache"      # ç¼“å­˜ç¼–è¯‘æ¨¡å‹
}

model = OVModelForCausalLM.from_pretrained(
    "models/openvino/gpt2",
    device="GPU",
    ov_config=ov_config,
    compile=True
)
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹å¤„ç†å¯ä»¥æå‡ååé‡
inputs = tokenizer(
    ["Hello", "How are you", "What is AI"],
    return_tensors="pt",
    padding=True
)

outputs = model.generate(**inputs, max_new_tokens=30)
```

---

## NPUé™åˆ¶è¯´æ˜

### ä¸ºä»€ä¹ˆNPUä¸å·¥ä½œï¼Ÿ

Intel NPU (AI Boost) éœ€è¦**å›ºå®šè¾“å…¥å½¢çŠ¶**ï¼Œä½†GPT-2ä½¿ç”¨**åŠ¨æ€å½¢çŠ¶**ï¼š

```
é”™è¯¯: to_shape was called on a dynamic shape
```

### å“ªäº›æ¨¡å‹æ”¯æŒNPUï¼Ÿ

NPUæœ€é€‚åˆï¼š
1. **å›ºå®šè¾“å…¥çš„æ¨¡å‹**: BERT, DistilBERT (åˆ†ç±»ä»»åŠ¡)
2. **å›¾åƒæ¨¡å‹**: ResNet, MobileNet, EfficientNet
3. **å°å‹ç¼–ç å™¨**: Sentence transformers

**ä¸é€‚åˆNPU**:
- GPTç³»åˆ— (åŠ¨æ€ç”Ÿæˆé•¿åº¦)
- T5ç³»åˆ— (seq2seq)
- å¤§å‹è¯­è¨€æ¨¡å‹

---

## æ¨èé…ç½®

### å¼€å‘ç¯å¢ƒ

```python
# config/openvino.yaml
openvino:
  device: "GPU"  # ä½¿ç”¨iGPU
  cache_dir: "models/cache"
  performance_hint: "LATENCY"
  num_streams: 1
```

### ç”Ÿäº§ç¯å¢ƒ

```python
# å¤šè®¾å¤‡é…ç½®
devices = ["GPU", "CPU"]  # iGPUä¼˜å…ˆï¼ŒCPUå¤‡ç”¨

for device in devices:
    try:
        model = OVModelForCausalLM.from_pretrained(
            "models/openvino/gpt2",
            device=device,
            compile=True
        )
        print(f"âœ… Loaded on {device}")
        break
    except Exception as e:
        print(f"âŒ {device} failed: {e}")
        continue
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ¨¡å‹é‡åŒ–

```bash
# å¯¼å‡ºINT8é‡åŒ–æ¨¡å‹
optimum-cli export openvino \
  --model gpt2 \
  --task text-generation \
  --weight-format int8 \
  models/openvino/gpt2-int8
```

**é¢„æœŸæå‡**:
- å†…å­˜: 75%å‡å°‘
- é€Ÿåº¦: 1.5-2xæå‡
- è´¨é‡: è½»å¾®ä¸‹é™

### 2. é™æ€å½¢çŠ¶ä¼˜åŒ–

```python
# å›ºå®šè¾“å…¥é•¿åº¦å¯ä»¥æå‡æ€§èƒ½
model = OVModelForCausalLM.from_pretrained(
    "models/openvino/gpt2",
    device="GPU",
    compile=True,
    # å›ºå®šå½¢çŠ¶é…ç½®
    input_info=[
        ("input_ids", [1, 128]),      # batch=1, seq_len=128
        ("attention_mask", [1, 128])
    ]
)
```

### 3. ç¼“å­˜ç¼–è¯‘æ¨¡å‹

```python
# é¦–æ¬¡ç¼–è¯‘åç¼“å­˜
ov_config = {
    "CACHE_DIR": "models/cache/gpu"
}

# åç»­åŠ è½½ä¼šå¿«å¾ˆå¤š
model = OVModelForCausalLM.from_pretrained(
    "models/openvino/gpt2",
    device="GPU",
    ov_config=ov_config
)
```

---

## ä¸CPUåŸºå‡†å¯¹æ¯”

### å½“å‰æ€§èƒ½ (OpenVINO)

| è®¾å¤‡ | TTFT | Tokens/s | å†…å­˜ | åŠ è½½æ—¶é—´ |
|------|------|----------|------|----------|
| CPU (PyTorch) | 1,397ms | 27-34 | 730MB | å¿« |
| CPU (OpenVINO) | 522ms | 57 | æœªæµ‹ | 0.49s |
| iGPU (OpenVINO) | 522ms | 57 | æœªæµ‹ | 13s |

**æå‡**: OpenVINO CPUæ¯”PyTorch CPUå¿« **2.7x** ğŸ‰

---

## ä¸‹ä¸€æ­¥ä¼˜åŒ–

### ç«‹å³å¯åš

1. âœ… **ä½¿ç”¨OpenVINO CPU** - å·²ç»æ¯”PyTorchå¿«2.7x
2. ğŸ”„ **æµ‹è¯•INT8é‡åŒ–** - é¢„æœŸå†æå‡1.5-2x
3. ğŸ”„ **ä¼˜åŒ–iGPUé…ç½®** - è°ƒæ•´æ€§èƒ½å‚æ•°

### æœ¬å‘¨å¯åš

4. **å®ç°æ¨¡å‹ç¼“å­˜** - å‡å°‘åŠ è½½æ—¶é—´
5. **æ‰¹å¤„ç†ä¼˜åŒ–** - æå‡ååé‡
6. **åŠ¨æ€è®¾å¤‡é€‰æ‹©** - iGPU/CPUè‡ªåŠ¨åˆ‡æ¢

### é•¿æœŸä¼˜åŒ–

7. **å°è¯•å…¶ä»–æ¨¡å‹** - DistilGPT2, GPT-Neo
8. **ONNX Runtime** - å¯¹æ¯”æ€§èƒ½
9. **DirectML** - WindowsåŸç”ŸGPUåŠ é€Ÿ

---

## æ€»ç»“

### å½“å‰æœ€ä½³æ–¹æ¡ˆ

**OpenVINO CPUæ¨¡å¼**:
- é€Ÿåº¦: 57 tokens/s (vs PyTorch 27-34)
- æå‡: **2.7x** ğŸš€
- ç¨³å®šæ€§: 100%
- æ¨è: âœ… ç«‹å³ä½¿ç”¨

### æœªæ¥ä¼˜åŒ–æ½œåŠ›

| ä¼˜åŒ– | é¢„æœŸæå‡ | éš¾åº¦ |
|------|----------|------|
| INT8é‡åŒ– | 1.5-2x | ğŸŸ¢ ç®€å• |
| iGPUè°ƒä¼˜ | 1.2-1.5x | ğŸŸ¡ ä¸­ç­‰ |
| æ‰¹å¤„ç† | 1.5-2x | ğŸŸ¢ ç®€å• |
| **æ€»è®¡** | **3-6x** | - |

**æœ€ç»ˆç›®æ ‡**: 150-300 tokens/s (vs å½“å‰27-34)

---

## ä½¿ç”¨ç¤ºä¾‹

### é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

```python
# mm_orch/runtime/openvino_manager.py
from optimum.intel import OVModelForCausalLM
from pathlib import Path

class OpenVINOManager:
    def __init__(self, device="CPU"):
        self.device = device
        self.models = {}
    
    def load_model(self, model_id, model_dir=None):
        """åŠ è½½OpenVINOæ¨¡å‹"""
        if model_id in self.models:
            return self.models[model_id]
        
        if model_dir is None:
            model_dir = f"models/openvino/{model_id}"
        
        model = OVModelForCausalLM.from_pretrained(
            model_dir,
            device=self.device,
            compile=True
        )
        
        self.models[model_id] = model
        return model
    
    def generate(self, model_id, inputs, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬"""
        model = self.load_model(model_id)
        return model.generate(**inputs, **kwargs)
```

### é…ç½®æ–‡ä»¶

```yaml
# config/optimization.yaml
optimization:
  backend: "openvino"  # ä½¿ç”¨OpenVINO
  device: "CPU"        # CPUæ¨¡å¼ (æœ€ç¨³å®š)
  fallback: ["GPU", "CPU"]  # å¤‡ç”¨è®¾å¤‡
  
  openvino:
    cache_dir: "models/cache"
    performance_hint: "LATENCY"
    num_streams: 1
    
  quantization:
    enabled: false  # ç¨åå¯ç”¨
    format: "int8"
```

---

**åˆ›å»ºæ—¶é—´**: 2026-01-28 18:20  
**çŠ¶æ€**: âœ… å·²éªŒè¯  
**ä¸‹ä¸€æ­¥**: æµ‹è¯•INT8é‡åŒ–
