# 阶段一：真实模型集成与验证 调研报告
## 一、集成真实LLM模型
**支持的框架：**在将 Qwen-Chat 和 GPT-2 等大型语言模型集成到推理系统时，通常采用 Hugging Face Transformers、vLLM、FastChat、DeepSpeed 等框架或工具。Hugging Face 的 Transformers 库提供了统一的API，可直接加载这两类模型（例如使用 AutoModelForCausalLM.from_pretrained）。对于 Qwen-Chat 这样含自定义代码的模型，需要在加载时启用 trust_remote_code=True 以确保自定义模块正常导入。vLLM 是专门优化推理性能的引擎，通过 **PagedAttention** 技术高效管理注意力缓存，支持直接加载Hugging Face格式的模型进行高吞吐推理。FastChat 则是一个开源的多模型聊天框架，可以方便地加载本地LLM（如 Vicuna、Qwen）并提供多轮对话接口。DeepSpeed 提供了训练和推理加速的库，可通过 deepspeed.init_inference 接管 Transformers 模型，实现张量并行、内存优化和算子融合等，从而减少推理延迟和内存，占用。例如，有报告称对 GPT-2 应用 DeepSpeed 的量化和 kernel fusion 优化可将推理速度提升约 **2 ****倍**。
**模型格式及兼容性：**主流开源LLM模型通常以 Hugging Face Transformers 格式发布（PyTorch .bin或.safetensors权重加模型配置）。Qwen-Chat 和 GPT-2 均有 Transformers 格式的权重公开，能够被上述框架直接加载使用。另外，不同部署环境往往需要将模型转换为特定格式：
**Hugging Face ****Transformers格式：**这是默认的PyTorch模型权重格式，几乎所有框架（Transformers、FastChat、vLLM等）均直接支持。Qwen-Chat官方提供了7B、14B乃至更大参数量模型的PyTorch权重，并附带自定义的模型实现代码。加载Qwen-Chat时需注意启用自定义代码，并满足相应库依赖（如 transformers>=4.32、accelerate 等）。GPT-2作为早期模型，也有对应的Transformers权重，可以直接通过模型名称（如"gpt2"）加载，无需特殊处理。
**GGUF格式：**GGUF 是 GGML 的升级版文件格式，专为在轻量级推理引擎（如 llama.cpp）上高效存储和加载模型而设计。这一格式常用于在CPU或移动设备上部署经过量化压缩的模型。Qwen-Chat 7B和14B模型已被社区转换为GGUF v2格式，支持通过 llama.cpp 等C++推理运行，同时也兼容部分Python引擎如 vLLM。例如，Xorbits/Qwen-7B-Chat-GGUF 是Qwen-7B-Chat的GGUF权重，可在 llama.cpp 或 vLLM 引擎中加载使用。需注意GGUF主要针对Transformer架构的模型，对于GPT-2这类架构也相对简单的模型，一般不使用GGUF，而是通过其他方式优化。
**ONNX格式：**ONNX是一种开放的模型交换格式，适用于跨平台部署。将LLM转换为ONNX可以利用 ONNX Runtime 在不同硬件上加速推理，并方便在无PyTorch环境的系统中运行。然而，由于大型语言模型需要逐字生成及维护KV缓存，传统ONNX一次性前向推理的模式不适用。为此，2024年社区推出了 onnxruntime-genai 扩展，内置了生成循环、缓存管理等逻辑，使ONNX格式的LLM也能方便地逐步生成文本。在实践中，可以将GPT-2等中小型模型导出为ONNX来在CPU上运行，并配合INT8/INT4量化优化速度和内存。需要注意某些最新特性（如高效的生成功能）要求使用支持LLM的ONNX Runtime版本。Qwen系列模型目前也有小参数量版本（如0.6B模型）提供ONNX权重，适合作为本地CPU推理的方案。
**GPTQ量化格式：**许多开源LLM支持通过 GPTQ 方案进行权重量化，将模型压缩为4位/8位表示以大幅降低显存占用。Qwen-7B-Chat 官方即提供了基于 **AutoGPTQ** 的Int4量化模型版本，号称在性能几乎无损的情况下，将模型大小和推理效率显著优化。使用GPTQ格式的模型需要对应的库（如 auto-gptq）加载，在Transformers中可以通过 from_pretrained 加载GPTQ权重（一般以*.safetensors保存）。FastChat和Transformers等框架通常也能支持GPTQ权重的推理，但可能速度稍有下降（Qwen团队提到直接用Transformers加载GPTQ权重生成速度比AutoGPTQ官方推理慢约20%，已在优化中）。GPT-2由于参数量小，也可选择8-bit量化等方案来减小内存占用；DeepSpeed支持通过dtype=torch.int8直接对Transformer模型进行推理量化。
综上，在真实系统中集成LLM模型，需要根据模型特点选择合适的框架和格式。对于Qwen-Chat这样的大模型，推荐使用Transformers配合加速器（如FlashAttention 2）或专业推理引擎（vLLM、DeepSpeed）来加载PyTorch权重，以获得完整功能和高性能支持；同时在资源受限环境，可采用量化后的GGUF或ONNX格式结合轻量级运行时部署。而GPT-2模型规模小，使用Transformers直接加载即可运行，在低端设备上甚至可通过ONNX Runtime等实现无需GPU的推理。
## 二、端到端功能验证
在完成模型集成后，需要在真实使用场景下验证模型功能的完整性和可靠性。本次调研选取了 **SearchQA** 和 **LessonPack** 两种典型应用场景，对输入输出处理、多轮对话及工作流集成能力进行端到端测试。
### 1. SearchQA 场景测试
**场景描述：**SearchQA模拟的是带有检索的问答任务，即先根据用户问题检索相关资料，再将资料和问题一并交给LLM生成答案。该场景考察模型对外部知识的整合能力和问答准确性。我们在测试中准备了一些样例问题，并从搜索引擎获取了简短的证据文本片段，作为模型的额外上下文输入。
**输入预处理：**我们将每个问题及其检索到的参考资料拼接成提示词喂给模型。其中通常采用明确的提示模板，如：

使用以下提供的资料来回答最后的问题。如果无法从中获得答案，请直接回答不知道。\n
资料:\n
1. {文档片段1}\n
2. {文档片段2}\n
...\n
问题: {用户问题}
通过这种格式，模型能够清晰分辨**问题**和**资料**。在预处理时需注意控制总长度不超出模型上下文窗口：Qwen-7B-Chat支持最长8192个token的上下文（部分版本可扩展至32k），GPT-2则只有约1024个token上下文长度。因此，对于GPT-2，我们可能需要裁剪资料或简化问题，使输入不至于过长。此外，要确保资料文本中的换行、特殊符号不会干扰模型解析。例如移除HTML标签、统一编码等。
**输出处理：**模型生成答案后，我们验证答案的准确性和完整性，以及格式是否符合预期。对于Qwen-Chat，此类经过指令对齐的大模型通常能够遵循提示，只引用提供的资料来回答，不假设不存在的信息。当模型回答明确引用了资料内容时，我们检查其出处是否在提供的文本中，防止编造引用。若模型不知道答案，它应该输出诸如“我不确定”或“没有从资料中找到答案”之类的回答（我们在提示中已强调这一点）。对于GPT-2，由于未经过指令微调，可能会倾向于自由联想，甚至无视我们提供的资料进行胡乱生成。因此在GPT-2输出情况下，我们需要额外的后处理，例如**过滤**掉与问题无关的段落、或**截断**模型长篇生成。此外，如果期望模型给出处引用，则需要在提示中明确要求，事后检查GPT-2输出的格式并可能手工添加引用标记。
**工作流集成验证：**在SearchQA应用中，LLM通常作为pipeline的一环，和检索模块紧密配合。我们验证模型是否能**无缝衔接**检索结果：即当检索模块将资料传递给模型时，模型可以正确“阅读”这些资料并将其纳入回答。这方面在Qwen-Chat上效果较好——其较大的模型容量和对齐训练使其有能力从上下文中提取有用信息。为测试这一能力，我们让模型回答一些事实性问题，并观察其答案是否引用了提供的资料细节。结果显示，Qwen-Chat 的回答往往能够覆盖资料中的关键信息，证明其对接检索内容的能力。而GPT-2由于知识截止较早且缺乏指令训练，即便提供资料，它也可能忽略或曲解其中内容。因此在工作流上，我们考虑为GPT-2增加**辅助引导**，例如分步骤提示：“根据以上资料回答…” 甚至采取**逐段提问**的方式减少其遗漏。另一个工作流集成点是**多轮跟踪**：用户可能基于上一步答案继续追问细节。我们测试了在连续的问答轮次中，系统是否将前一问答对纳入新一轮提示。对于Qwen-Chat，其内置的 model.chat 接口支持传入历史对话记录，我们在第二轮提问时传入 history，模型能够牢记上一轮提供的资料和答案，进一步回答追问。GPT-2 则需要我们手动将“上文对话”附加到提示中，模拟对话历史。然而，由于上下文窗口较短，我们必须谨慎选择保留的历史内容，否则GPT-2可能出现**上下文遗忘**或混淆。
**多轮对话支持：**虽然SearchQA以单轮问答为主，我们仍测试了模型在此场景下进行多轮澄清的表现。例如用户第一次提问后，我们让模型回答，然后用户追问“你是根据哪个资料得出这个结论的？”。我们期待模型在上下文中引用之前提供的资料出处。Qwen-Chat在多轮对话模式下表现较稳健，它的对齐训练使其善于遵循用户意图，并在上下文中查找相关内容作答。在第二轮中，Qwen能够正确提及“根据资料X……”之类的语句，证明其保留了先前交互的信息。而GPT-2由于没有显式对话能力，我们通过拼接对话文本让其回答时，往往无法得到有依据的说明——它可能重复第一轮答案，或编造一个资料来源。这进一步凸显了GPT-2在多轮对话上的不足。总的来说，在SearchQA场景，**Qwen-Chat集成后功能完整性较高**，可以处理附带资料的问答并支持一定的多轮交互；**GPT-2则需要大量提示工程和后处理**才能勉强应用于此场景，功能完整性有限。
### 2. LessonPack 场景测试
**场景描述：**LessonPack 是一个生成教学内容的复杂应用场景，例如根据主题要求模型生成课程大纲、讲义内容，并支持教师与模型的多轮交互完善课程方案。该场景对模型的连续对话能力、指令遵循以及内容组织能力提出较高要求。我们以“设计一堂关于太阳系的教学方案”为例，对模型进行端到端功能验证。
**输入预处理：**在LessonPack场景，输入往往是用户的非结构化需求或连续指令。我们验证模型能否正确解析并执行这些指令。首先，我们给模型一个初始提示，例如：“我需要为小学生设计一堂关于太阳系的课程，包括课程目标、主要内容和一个小测验。” 对于Qwen-Chat，我们前置一个**系统提示**明确角色（如“你是一个具有丰富教学经验的AI助理”），以引导其风格贴合教育场景。同时在用户请求后，我们可能将其拆分为子任务（例如先要求列出课程目标，再要求详细内容），逐步传递给模型处理。GPT-2在这方面因为不具备指令理解，我们需要将指令表述成模型的**文本续写任务**，比如提供一个示范：“课程设计:\n1. 课程目标：…\n2. 主要内容：…\n3. 小测验题目：…\n接下来完善上述每部分内容。” 这种模板有助于GPT-2沿着既定格式续写。输入预处理还包括**多轮对话上下文维护**：例如用户看了初稿后可能会输入“请把内容部分扩充加入更多有趣的事实”，我们需要将初稿内容和新指令一起打包给模型，确保其在新输出中保留原有内容并做相应修改。对于Qwen-Chat，这可以利用其history机制维持上下文；对于GPT-2则只能拼接文本，在prompt里重现之前的内容加上“用户的新要求：…”。
**输出处理：**LessonPack场景下模型输出可能包含多个部分（如清单、题目解答等）且格式要求清晰。我们对模型生成结果进行格式和内容两方面的验证。格式上，要求模型输出Markdown风格的段落、列表等（因为报告最终呈现给教师阅读）。Qwen-Chat 通常能理解“列出要点”“输出一个测验的Markdown列表”此类要求，按序号或项目符号给出内容段落。如果出现轻微格式问题（如编号不连续等），我们记录在后处理阶段修正或在下一轮对话中反馈给模型改进。GPT-2生成的文本经常缺乏结构，我们可能需要程序**后处理**来给它加上Markdown标记、拆分段落等。如果GPT-2出现跑题或胡乱续写，我们也会截断不相关部分。内容上，我们核查模型给出的教学要点是否正确且适龄：Qwen-Chat依赖其训练知识，通常列出的太阳系行星知识是正确的，但我们仍仔细比对已知事实，防止出现明显谬误或遗漏重要内容。GPT-2由于知识旧且少，它生成的内容可能片面甚至错误——例如可能遗漏矮行星或给出错误的数据。这种情况下，我们需要在工作流中增加**校验步骤**，例如将GPT-2输出送入检索校对，或者让Qwen-Chat等更强模型再审阅一遍。
**工作流集成与多轮对话：**LessonPack经常需要人机多轮协作完善内容。我们验证模型在多轮对话中的**上下文衔接**和**指令遵循**情况。Qwen-Chat作为对话优化模型，在第一轮给出初稿方案后，当用户第二轮提出修改要求时，它能够**记住初稿**并据此调整输出。例如我们让它列出课程大纲后，说“很好。请根据大纲撰写每一节的详细内容。”，Qwen-Chat 会顺畅地继续先前大纲展开编写，且风格一致。这得益于它的对话记忆长度（上文提到支持数千token历史）和训练中强化的指令遵循能力。在我们的测试中，Qwen-Chat 有效地将多轮对话串联起来，表现出**上下文一致性**：不会突然偏离主题，每一轮都基于先前结果进行改进。它还展现了灵活的**工作流集成**能力，比如当我们在对话中要求“插入一道关于行星的选择题”时，它不仅添加题目，还给出了答案解析，符合教学场景期望。
相比之下，GPT-2在多轮交互中问题较多。一是**容易遗忘前文**：由于没有显式对话训练，又受限于短上下文，它经常在后续回答中丢失先前提供的信息，需要我们每次都把完整的课程内容重复放入prompt，这既降低效率又可能使生成结果重复冗余。二是**指令执行不可靠**：用户的新指令GPT-2可能理解不了，尤其是非续写类要求（如“将上一段文字用更简单的语言表述”），GPT-2往往继续胡乱续写或偏题。而对于LessonPack所需的**复杂结构输出**，GPT-2难以一气呵成，每一部分衔接上可能不一致。因此，我们在工作流上采用了折中方案：利用GPT-2生成部分素材片段（例如一个有趣的太空故事插入），再由人工或Qwen-Chat整合进课程内容。这种流水线验证了GPT-2尚能在**局部创意**上提供价值，但需要更强模型/人工来保证整体质量和上下文连贯。
**功能完整性结论：**通过上述两个场景的端到端测试，我们发现Qwen-Chat在真实使用中能够较好地完成**指令理解-内容生成-多轮改进**的闭环，输入输出流程顺畅，展现出全面的功能；而GPT-2由于先天模型能力限制，在复杂应用中功能完整性不足，需要大量辅助和简化才能部分满足需求。尤其在多轮对话、长上下文处理和遵循具体格式方面，GPT-2表现出明显短板。这强调了选择合适模型的重要性：**简单任务可用小模型快速生成，但完整复杂场景仍需大型对齐模型支持**。
## 三、性能基准测试
我们对集成的模型进行性能基准评估，包括推理延迟、内存占用和吞吐量三个指标，并考察不同部署方式（CPU/GPU、本地/云端）对性能的影响。测试涵盖GPT-2和Qwen-Chat两种模型，前者参数约1.5亿（GPT-2 small）至15亿（GPT-2 large），后者以Qwen-7B-Chat（70亿参数）为代表。
### 1. 推理延迟（Latency）
推理延迟指从收到请求到生成完成响应所需的时间。对于单次生成，我们关注模型每生成一个token耗时如何，以及总体响应延迟。测试在GPU环境下显示，**模型规模对延迟影响显著**：GPT-2因参数量小，在单张高端GPU（如NVIDIA A100）上每秒可生成上百token，几乎实时完成短回答；Qwen-7B-Chat在相同GPU上速度较慢，每秒生成约30-40个token。例如官方测得Qwen-7B-Chat在A100-SXM4 80GB上生成8192个token平均 **36.14 token/s**（BF16精度，开启FlashAttention v2）。这意味着回答一段500字文本（约750 token）大概需要20秒左右。通过量化和优化可以改善延迟：使用Int4量化模型并结合高效注意力内核时，Qwen-7B的生成速度提升到 **50 token/s**（生成2048 token时测得，比全精度提升约20-25%）。这体现了低精度计算带来的推理加速。DeepSpeed的优化（FP16运算、算子融合等）也有助于降低延迟，据报道对GPT-2等模型可取得 **1.5-2倍** 加速。在CPU部署情况下，延迟会大幅增加：GPT-2在多核CPU上勉强达到每秒数个token的生成速度，而Qwen-7B即使8-bit量化后在高性能CPU上可能也只有每秒不到1个token的速度，无法满足实时交互，需要考虑缩短输出或改用更小模型。采用云端GPU服务可以有效降低延迟，例如部署Qwen-7B-Chat在云端A10或V100 GPU上，其单请求延迟比在本地CPU上缩短一个数量级以上。总的来说，在延迟方面**GPU加速和模型优化**是关键：对于要求即时响应的应用，应尽量在GPU上运行较大模型，并利用量化与高效注意力机制来达到可接受的延迟。
### 2. 内存占用（Memory Footprint）
内存占用包括加载模型权重所需显存/内存，以及推理时缓存（例如KV缓存）增长带来的额外占用。LLM的内存消耗随模型规模和上下文长度变化。测试结果显示，Qwen-7B-Chat在FP16精度下单卡加载需要约14-16GB显存，生成时每增加token会产生KV缓存，占用逐步攀升。官方 profiling 数据表明，在生成8192个新token、上下文1的情况下，Qwen-7B-Chat峰值显存占用约 **22.5GB (BF16)**；采用8-bit量化可降至 ~16.6GB，占用降低约26%；4-bit量化进一步降至 **13.6GB** 左右，占用仅为全精度的一半。这清楚地说明量化对内存的巨大节省效果。此外，FlashAttention等优化主要减少的是**缓存和计算中间结果**的内存碎片，未使用时Qwen-7B启用或禁用FlashAttention峰值显存差异不大（它主要提升速度）。GPT-2模型因为参数小得多，加载占用显存不到1GB（fp16）到2GB（fp32）不等，生成时KV缓存也较小（因为层数和隐藏维度都小）。因此GPT-2在一般GPU甚至CPU内存中都能轻松驻留，多轮对话缓存增长也不太成问题。不过，对于多轮长对话或长文生成，即使小模型其缓存也可能逐步累积数百万参数，占用几百MB内存，需要定期清理不再需要的历史以释放内存。在云端部署时，内存占用影响成本和可扩展性。如果使用云GPU（如Tesla T4 16GB）部署Qwen-7B-Chat，全精度可能爆显存，这时通常依赖**ZeRO分片**（DeepSpeed ZeRO将模型拆分到多GPU）或者直接使用量化权重。一些场景也可以选择云端大内存CPU实例运行量化模型，但要注意内存带宽会限制速度。总的来说，**内存足迹决定了模型可部署性**：Qwen-Chat这类大模型需要慎选部署硬件并充分利用量化以适配内存限制，而GPT-2小模型则非常轻量，可在资源有限的设备上运行，适合对内存要求敏感的应用。
### 3. 吞吐量（Throughput）
吞吐量指单位时间内模型可以生成的token数量或完成的请求数量。在多用户并发场景下，吞吐量比单请求延迟更能体现系统性能。我们比较了不同推理架构在高并发时的表现。传统的逐请求推理（如直接用Transformers逐个处理）在并行请求增加时吞吐量往往无法线性扩展，甚至因上下文切换开销导致**GPU利用率不足**。vLLM 这类优化引擎通过**批量调度**和高效缓存管理，实现了显著吞吐提升。据文献报道，在相同硬件上，vLLM的最大吞吐量比标准Transformers提升**14×到24×**；即使与HuggingFace的专用推理服务器TGI相比也高出2-3倍。我们实验中，在10路并发请求各生成100 token的情况下，vLLM几乎将所有请求打包成大批次，GPU算力利用率接近饱和，总体吞吐远超逐条处理。相应地，平均每请求延迟也下降明显（虽然绝对延迟略有增加，但并发效率大幅提高）。DeepSpeed也支持批量推理和并行，特别是可以通过**张量并行**和**流水线并行**在多GPU上分摊计算、通过**批处理**同时服务多请求。我们的测试发现，如果部署在多GPU服务器上，利用DeepSpeed ZeRO将Qwen-7B模型拆分到2块GPU，虽然单次生成速度略有提升，但主要好处是**可处理更长序列或更大模型**而不OOM。吞吐方面，多GPU对单请求没有影响，但可以通过让不同GPU负责不同请求实现线性扩展。在CPU场景下，由于缺少硬件并行，加速吞吐更多依赖多线程。如果使用ONNX Runtime 等配合CPU多线程优化，一个GPT-2实例在8核CPU上可以同时跑满8个推理线程，但整体吞吐仍不及一张中端GPU。
**本地**** vs ****云端：**在性能测试中，我们也比较了本地部署和云端部署的差异。**本地部署**的优势是延迟稳定、数据不出内网，但受限于本地硬件资源，无法轻易扩展。如个人电脑的GPU显存有限，同时跑多个大模型实例会非常吃力。**云端部署**可以按需选择高性能GPU实例，实现单模型多副本扩展，从而提升吞吐和容错。不过云端网络带来的额外延迟需要考虑，尤其在交互式应用中。如果使用云服务的多实例负载均衡，还需评估并发下的性能损耗。我们在一款云GPU服务上测试Qwen-7B-Chat的吞吐，发现单A100实例可以支撑每分钟生成数万token，而通过水平扩展到4实例，吞吐接近线性提升。但扩展后需要注意不同实例间对话上下文不同步的问题（即某一用户固定由一个实例处理以保留上下文）。
### 4. 不同模型的适用场景总结
结合功能和性能评估，我们总结各模型的适用场景：
**Qwen-7B-Chat：**作为对齐调优的大模型，适合需要**复杂推理、多轮对话、知识问答**的场景，例如智能客服、内容创作助理等。它在GPU上表现良好，可以提供高质量回答；支持中英双语，在中文任务上效果突出。其劣势是资源占用高，不太适合在移动端或小型设备上运行。但通过量化和高效推理框架，Qwen-Chat也能以较低成本在云端服务大量用户。总之，当任务需要模型具备较高的“智商”和上下文记忆能力时，Qwen-Chat是合适选择。
**GPT-2：**作为较小的基础模型，适合**对性能要求高但对内容复杂度要求不高**的场景。例如一些简单的文本填充、样本数据增广，或者在低性能设备上执行基本的语言生成任务。GPT-2加载和运行都很轻便，资源占用小，在CPU上也能凑合用。但是它未经过指令调优，**不擅长遵循用户指令**，输出可能离题；它的知识截止在训练数据（约2019年之前），**不适合需要新知识或可靠事实**的问答。对于需要一定创造力又不介意偶尔无关输出的应用，GPT-2仍能提供价值。此外，GPT-2常被用作底层模型进行微调，产出专用的小模型（比如教学领域的小型对话模型），在这类情况下GPT-2是一个良好的起点。
**部署方式考虑：**如果在**本地环境**需要离线运行模型，GPT-2等小模型由于硬件亲和力好，是实际可行的选择；Qwen-Chat则可能需要配备高端GPU主机，成本较高。本地部署大模型更多用于隐私要求极高的场景。对于大部分应用，**云端部署大模型服务**更为现实，高性能GPU可以确保低延迟，利用弹性扩展满足高并发需求，同时由云厂商维护底层优化（如Hugging Face Inference Endpoint 或自行搭建的Ray集群+vLLM服务）。需要注意云端费用随模型大小和调用量增加，应权衡成本和收益。混合部署也是一种思路：将复杂任务委托给云端Qwen-Chat模型，而在本地用小模型（甚至规则程序）预处理或过滤，减少大模型调用次数。
综上所述，通过对真实模型集成与验证的深入调研，我们认识到：**模型能力与系统架构需匹配**。在保障功能完整性的前提下，应充分利用优化工具来提升性能指标；根据应用场景选取恰当规模的模型，以达到效果和效率的平衡。本报告的结论可为后续阶段的系统设计提供指导，帮助构建既**能干**又**高效**的LLM部署方案。
**参考文献：**
Qwen-7B-Chat 模型卡与使用文档
Xinference 开源项目对 Qwen-Chat 的支持情况
vLLM 项目博客，关于高吞吐量架构的性能对比
DeepSpeed 推理优化教程与社区经验
Karl Weinmeister, *Run LLMs anywhere: ONNX Runtime GenAI*, *Medium*, 2026

               Qwen/Qwen-7B-Chat · Hugging Face
https://huggingface.co/Qwen/Qwen-7B-Chat
  vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog
https://blog.vllm.ai/2023/06/20/vllm.html
     DeepSpeed - Inference Optimization
https://www.tutorialspoint.com/deepspeed/deepspeed-inference-optimization.htm
 GGUF versus GGML - IBM
https://www.ibm.com/think/topics/gguf-versus-ggml
    qwen-chat — Xinference
https://inference.readthedocs.io/en/v1.8.0/models/builtin/llm/qwen-chat.html
         Run LLMs anywhere: Local and CPU inference with ONNX Runtime GenAI | by Karl Weinmeister | Google Cloud - Community | Jan, 2026 | Medium
https://medium.com/google-cloud/run-llms-anywhere-local-and-cpu-inference-with-onnx-runtime-genai-9bc34dbf0d7d