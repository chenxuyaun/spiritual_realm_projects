## V5 总目标：从“一问一答流水线” → “会思考、会记忆的对话 Agent”

这一轮我们做三件大事：

1. **Self-Ask 复杂问答工作流**

   * 对复杂问题自动“自问自答、自行分解”，对每个子问题调用你现有的搜索/摘要/回答链路，然后汇总成最终答案。
   * 对标 Self-Ask 论文：模型先自己提出 follow-up 问题，再用搜索引擎回答，显著提升组合推理能力 ([arXiv][1])。

2. **真正的多轮对话 + 记忆模块**

   * 引入会话管理与记忆：短期上下文 + 长期记忆（基于摘要和向量检索）。
   * 对标近期对话记忆研究：递归摘要 + 检索式记忆，可以让模型在超长对话里保持一致性与个性化 ([arXiv][2])。

3. **让 Router 利用“对话 + 记忆”做更聪明的决策**

   * Router 不再只看单条 query，而是看：这是谁？是持续学习？还是一次性搜索？历史上用哪个 workflow 效果最好？
   * 继续用你的 trace + reward 机制，给 Router 再“喂一轮经验”。

（可选加强版：在 write_story 上试一点点 Tree-of-Thoughts 风格“多分支构思 + 自评选优”，ToT 被证明在创意写作和复杂规划上比普通 CoT 强很多 ([OpenReview][3])。）

---

## 任务一：Self-Ask 复杂问答工作流（self_ask_search_qa）

### 1.1 背景：Self-Ask 的思路

Self-Ask 的核心：

> 模型**先**判断“还需不需要后续问题”，如果需要，就显式写出 follow-up 子问题，再用搜索/工具回答子问题，最后整合出最终答案。([arXiv][1])

这跟你现在的架构非常契合：

* 子问题可以交给现有的 `search_qa` 或 `search_qa_fast` workflow。
* 你已有 bandit+Router，可以以后用 Self-Ask 的效果作为奖励信号。

### 1.2 新 Workflow：`self_ask_search_qa`

在 `WorkflowRegistry` 中新增 workflow：

* 名称：`self_ask_search_qa`
* 场景：问题包含“多跳/多实体/复杂推理”的情况（如“先…再…谁更…？”）

推荐 Step 组合：

1. `SelfAskDecomposeStep`

   * 输入：`state.question`
   * 输出：

     * `state.self_ask_plan`: List[子问题描述]
   * 实现方式：

     * 使用生成模型（现有 generator）+ Self-Ask few-shot Prompt：

       * Question: ...
       * Are follow up questions needed? Yes/No
       * Follow up: ...
       * Intermediate answer: ...
     * 只在这里用到 “Follow up:” 部分问题，不在这个 step 里回答。

2. `SelfAskSubQAExecutionStep`

   * 输入：`state.self_ask_plan`
   * 对每个子问题：

     * 通过内部调用 runner 或直接调用 `search_qa` workflow（可以限制为 `search_qa_fast` 减少成本）
   * 输出：

     * `state.self_ask_intermediate_answers`: List[(sub_q, answer, citations)]

3. `SelfAskAggregateStep`

   * 输入：

     * 原始 `question`
     * `self_ask_plan`
     * `self_ask_intermediate_answers`
   * 调用 generator，用一个聚合 Prompt：

     * 描述每个子问题及答案
     * 要求模型据此推理出最终答案，并在末尾列引用（聚合所有 citations）

### 1.3 Router 集成

* 在 Router v1 (规则) 中增加：

  * 若问题包含“谁更… / 先…然后… / 同时涉及两人/两个事件对比”等模式 → 推荐 `self_ask_search_qa`。
* 在 Router v2 训练数据中新增一个 label：`self_ask_search_qa`，从日志中筛选复杂问答样本打标。

---

## 任务二：多轮对话 Shell + 会话管理

### 2.1 新的“会话模式”入口

编写一个新的入口脚本，例如：

```bash
python -m mm_orch.chat
```

特性：

* 为每个会话生成 `conversation_id`
* 循环读取用户输入（直到输入 `/exit`）
* 每轮调用核心 API：

  * `ConversationManager.handle_turn(conversation_id, user_utterance)`
  * 返回：`assistant_reply` + `trace_id`

### 2.2 ConversationManager / 会话状态

新增模块 `mm_orch/conversation.py`：

* `ConversationManager` 负责：

  * 维护会话 turn 列表：`[(role, text, timestamp)]`
  * 负责调用 Memory 模块（下一节）
  * 为 Router 提供“当前会话摘要 + 最近几轮原文”
  * 封装调用 runner 的逻辑（根据 Router 选的 workflow）

---

## 任务三：记忆模块（Memory）——短期 + 长期

最新工作一般把对话记忆分为三类：

1. 只靠上下文拼接（无显式记忆）
2. 检索式记忆：对历史对话做 embedding，按相关度检索回填上下文
3. 记忆模块：对过去对话做摘要，甚至构建用户知识图谱，用作个性化记忆 ([arXiv][2])

你已经有向量库 + summarizer，很适合做一个轻量版 2+3 结合。

### 3.1 模块结构

新增 `mm_orch/memory.py`：

* `ShortTermMemory`

  * 最近 N 轮原始对话（比如 6~8 轮）

* `LongTermMemory`

  * 存储：

    * `memory_chunks`: 由对话+摘要形成的片段
    * 通过你的 embeddings + vector_store 管理（可单独一个 `.mem` 路径）

* `MemoryManager` 统一接口：

  * `update(conversation_id, new_turns)`
  * `retrieve(conversation_id, query, k)` → 返回相关记忆片段
  * `summarize_if_needed(...)`：在对话超过一定长度时触发“递归摘要”

### 3.2 递归摘要策略（参考论文做简化版）

论文《Recursively Summarizing Enables Long-Term Dialogue Memory in LLMs》提出：

* 先对局部短对话做小摘要
* 再用之前的摘要 + 新对话，递归生成更高层级摘要
* 最终用少量摘要作为长期记忆，让对话保持一致 ([arXiv][2])

你可以做一个工程版：

1. 每当对话长度 > M 轮：

   * 取最早的一批 turns（比如 10 轮）+ 当前已有 summary
   * 用 summarizer 生成一个新的 `session_summary`
   * 把这些旧 turns 丢进 LongTermMemory（embedding + 存储），只保留 summary 作为“短期概要”

2. `retrieve` 时：

   * 用当前问题作为 query，去 LongTermMemory vector_store 搜 top-k 片段
   * 这些片段连同最近短期对话一起，填入 workflow 的 State（例如 `state.memory_context`）

### 3.3 与 Workflow 的对接

* 在 ConversationManager 调用 Router 前：

  * 先从 MemoryManager 中取出相关记忆：`memory_context`

* Router 的特征中增加：

  * 是否存在 memory_context
  * memory_context 里的关键词（如“上次你让我讲…”，“继续上一节课”等）

* 在具体 workflow 的 Step 里：

  * 对 `search_qa` / `rag_qa`：可以把 memory_context 拼到 question 前面（带提示“之前的对话中你们讨论了：…”）
  * 对 `teach_explain`：可以让生成模型知道“这是同一个学生的下一节课”。

---

## 任务四：Router 利用记忆 + 多轮上下文（再升级一轮）

### 4.1 Router 接口扩展

扩展 Router 输入特征，至少加入：

* 当前 query 文本
* 最近 N 轮对话（或它们的 summary）
* memory_context 中的一些标志（是否是“继续/复习/延伸/上一次的内容”）

在 TF-IDF + LR 模型里可以这样做：

* 把 “最近对话 + query + memory 摘要” 拼成一个大文本喂给 TF-IDF
* 或者额外加几个手工特征（类似你现在的 keyword flags）：

  * `is_followup`（包含“继续”“上次”“刚才”“接着”等）
  * `is_learning`（“讲解”“教学”“练习”“作业”“复习”等）

### 4.2 训练数据更新

* 从新的会话 trace JSONL 中，按 conversation_id 聚合样本：

  * 同一会话内后续问题，标签中 `teach_explain` / `rag_qa` 等可能更优
* 更新 `train_router_from_logs.py`：

  * 支持把“最近几轮 utterances + memory 摘要”一起写入训练特征
  * 继续支持 `--label best_reward` 模式，让 Router 从奖励里学会“这类持续提问更适合走 teach_explain / rag_qa”。

---

## 任务五（选做）：给 write_story 加一点 Tree-of-Thoughts 味道

Tree-of-Thoughts（ToT）把普通的“思维链”升级为“思维树”，让模型能在多个候选推理路径之间探索、回溯，特别适合复杂推理和创意写作 ([OpenReview][3])。

你可以做一个轻量版 ToT-Story：

1. 在 `StoryPlanStep` 之后新增：

   * `StoryBranchStep`：

     * 对某一章节，让模型一次性提出 2–3 个可能的情节走向（“分支思路”）
   * `StoryBranchEvaluateStep`：

     * 用另一轮生成（或简单打分规则）评价每个分支的“有趣度/连贯性”，选择 1 个
     * 或者保留两个分支，让读者选（互动小说风格）

2. 再由 `StoryGenerateStep` 基于选中的分支继续写。

这会让你的 write_story 工作流成为一个小型 “思维树写作器”，和 ToT 在创意写作上的用法是同一条路子 ([OpenReview][3])。

