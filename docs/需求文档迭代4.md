## 总体目标（V4）：评测、体验和自我进化

1. **对人类友好**：CLI 输出要真正把 story / teaching / RAG 的内容打出来。
2. **让 Router 真正“从经验中学习”**：把 E3 训练链路打通，让新 Router 接管线上决策，并可对比老版本。
3. **做一套轻量但靠谱的评测框架**：尤其是对 RAG 和 teach_explain。RAG 评测可以参考近期对 RAG QA 的统一评测框架，结合检索和生成两个维度来打分 ([MDPI][1])。
4. **安全 & 稳定**：加入最小的内容/任务约束，让系统不会乱跑危险指令。

---

## 任务 1：CLI / 输出层优化（让新工作流“见光”）

> 目标：不同 workflow 有不同“主输出字段”，CLI 应该自动选对字段展示。

### 1.1 main.py：按 workflow 选择展示字段

在 `main.py` 的最终输出逻辑中：

* 如果 `workflow == "write_story"`：

  * 打印 `state.story_plan`（可选）
  * 打印 `state.story_text`（正文）
* 如果 `workflow == "teach_explain"`：

  * 打印 `state.teaching_plan`
  * 打印 `state.teaching_text`
  * 打印 `state.exercises`（带题目和答案）
* 如果 `workflow == "rag_qa"`：

  * 打印 `state.final_answer`
  * 打印命中的 `kb_sources`（类似 citations）

保留原本 search_qa 的 `final_answer + citations` 输出。

### 1.2 统一“标题 + 元信息”

为所有 workflow 的 CLI 输出加一个统一头部，例如：

```text
[Workflow] write_story
[Router]  router_v2 + bandit (arm=write_story, reward=0.82)
[Steps]   WebSearchStep -> FetchTopNStep -> ...
```

方便你之后看 trace 的时候对上号。

---

## 任务 2：文档导入脚本（RAG 真正可用）

> 目标：你能一条命令把本地的教学文档 / 团队规范 / 材料导入 `.kb`。

### 2.1 `scripts/ingest_docs.py`

功能大纲：

* 参数：

  * `--paths`：文件或目录（支持递归）
  * `--chunk-size`：默认 512 char
  * `--chunk-overlap`：默认 64 char
  * `--kb-path`：默认 `.kb`
* 流程：

  1. 遍历路径，支持 `.txt/.md/.pdf/.docx`（可先只做 `.txt/.md`，其他后续加）
  2. 读取文本 → 按 chunk-size+overlap 切块
  3. 调用 `mm_orch.tools.embeddings.get_embeddings()` 得到向量

     * 你已有 “sentence-transformers 优先，transformers 次之，离线零向量兜底”，继续沿用 ([科学直通车][2])
  4. 调用 `NpyVectorStore.add_texts(chunks, ids)` 写入 `.kb`
  5. 记录 `kb_sources` 中对应的 `doc_id` 和原文件名/路径

验收小例子：

```bash
python scripts/ingest_docs.py --paths ./docs/code_style.txt ./docs/teaching_notes/ --chunk-size 400
python -m mm_orch.main --workflow rag_qa "我们团队的代码规范大概有哪些要点？"
```

预期：

* rag_qa 能从刚导入的 `.kb` 中检索到片段
* final_answer 里体现出对应要点，kb_sources 列出 doc_id

---

## 任务 3：E3 真正落地——Router v2 从日志中“长脑子”

OpenAGI 的 RLTF 思路就是“用任务反馈去改进 LLM 的规划能力”([arXiv][3])。你已经有 reward + bandit + 训练脚本雏形，现在要做的是：

1. 把 E3 训练脚本打磨成可重复使用的 pipeline。
2. 在 main/router 里加上“新 Router 模型”的读入逻辑。
3. 写个小评测脚本，对比旧 Router vs 新 Router 的表现。

### 3.1 完善 `scripts/train_router_from_logs.py`

在已有雏形基础上，补齐以下逻辑：

1. 从 `.traces/*.jsonl` 中筛选训练样本：

   * question 非空
   * reward 有值
   * chosen_workflow 在你支持的 set 内（过滤旧实验脏数据）

2. 多种打标模式（通过参数指定）：

   * `--label chosen`：用运行时实际选择的 workflow 做监督
   * `--label best_reward`：对同一个 question 聚合，选 reward 最大的 workflow 做标签（更接近 RLTF 的“用结果挑更优规划”）([腾讯云][4])

3. 特征：

   * 文本：直接用 char n-gram / word n-gram 做 TF-IDF（你已经在用）
   * 也可以额外加一些 binary features：

     * 是否包含“最新/今天/上网/新闻/总结/写故事/讲解/教学/代码/规范”等关键词

4. 模型：

   * 保持 TF-IDF + LogisticRegression，快速可靠
   * 输出：`router_vectorizer.joblib` + `router_clf.joblib` + 每个 workflow 平均 cost 的统计（router_costs.json）

5. 日志：

   * 打印训练集 size、各类 workflow 的样本分布
   * 交叉验证准确率 / F1（粗略即可）

---

### 3.2 main / router：接入“新 Router 模型”

在 Router v2 中支持：

* 如果环境变量或配置中提供：

  * `ROUTER_VECTORIZER=router_vectorizer.joblib`
  * `ROUTER_CLF=router_clf.joblib`
* 则加载新模型作为 Router v2 的默认实现。
* 保留一个回退策略：加载失败或不存在 → 退回规则 Router v1 + bandit。

并在 trace 里写入：

* `router_version`: `"v2-tfidf-lr-YYYYMMDD"` 或 `"v1-rules"`

---

### 3.3 对比评测脚本：`scripts/eval_router_compare.py`

目标：给你一个数字：新 Router 到底比旧的好多少。

1. 输入：

   * `--questions questions.txt`：每行一个自然语言 query
   * `--runs 3`：每个问题用每个 Router 跑几次取平均 reward

2. 流程：

   * 对每个 question：

     * 用旧 Router 固定（禁用 bandit 更新）跑 N 次，记录平均 reward / cost
     * 切换到新 Router 固定（同样禁用 bandit 更新），再跑 N 次
   * 输出 CSV / Markdown 表格：

     * question / old_reward / new_reward / old_cost / new_cost / 优胜者

3. 指标设计：

   * 注意 cost 可以用你 trace 中已有的 `normalized_cost`
   * 也可直接算 “reward = quality - λ*cost”，看整体提升

这一步做完，你就拥有类似 OpenAGI 那种“用一整套标准任务评测控制器好坏”的能力，只是你的是轻量版的 ([arXiv][3])。

---

## 任务 4：RAG & teach_explain 的简单评测集

既然你教 K-5 艺术和音乐，也非常适合做一个**教育场景 RAG/讲解小基准**。最新文献表明，把 RAG 用在教育里，既能提高事实准确性，又能保持知识更新，是很有前景的方向([科学直通车][2])。

### 4.1 RAG QA 小基准（团队 or 课程）

* 建一个 `eval/rag_qa.jsonl`，结构类似：

  ```json
  {"q": "我们团队代码规范的缩进要求是什么？", "gold_keywords": ["4空格", "不使用tab"]}
  {"q": "课堂上讲过的印象派画家的代表人物有哪些？", "gold_keywords": ["莫奈", "雷诺阿"]}
  ```
* 评测脚本检查：

  * 是否触发 rag_qa 工作流
  * final_answer 中是否包含 gold_keywords
  * reward / cost

很多 RAG 评测框架都是“检索指标 + 生成指标”结合，比如 CoURAGE 框架会统一多个指标（检索覆盖率、答案相关性等）([Springer][5])。你可以先用这种“包含关键词 + RAG 是否被调用”的简单版，后面再迭代。

### 4.2 teach_explain 小基准（面向 K-5）

* 建一个 `eval/teach_explain.jsonl`：

  ```json
  {"q": "三年级学生理解的音阶是什么？", "must_sections": ["引入", "例子", "练习"]}
  {"q": "给四年级学生讲解四分音符和八分音符", "must_sections": ["概念", "节奏", "练习"]}
  ```
* 评测：

  * teaching_text 是否包含这些章节标题或关键词
  * exercises 是否数量达到 N 条以上
  * reward 记录下来

---

## 任务 5（可选）：参考最前沿的“多路由器 + 容错”思想

你现在的 Router 是“单一 Router + bandit 修正”，已经很实用了。
如果想玩一点研究味道的东西，可以参考最新的 **Mixture of Routers (MoR)**、Routing Experts 和 LLM-based router 的想法：

* MoR：用多个子路由器一起决定专家选择，再由主路由学会融合，改善鲁棒性和参数效率 ([arXiv][6])
* Routing Experts：在现有多模态 LLM 里动态路由专家，实现示例依赖的最优路径 ([CSDN 博客][7])
* LLMoE：直接用 LLM 做路由器，代替传统网络，让专家选择更可解释 ([arXiv][8])

你可以在未来的版本里尝试：

* 让 **多个简单 Router**（规则 / TF-IDF / 小 BERT 分类）各自打分
* 再用一个轻量“主 Router”融合结果（比如线性模型或小 MLP）
* 如果它们意见不统一，可以当成“不确定样本”，专门记录下来做人工/半自动标注

这部分暂时只当“研究向规划”，不必现在就实现。