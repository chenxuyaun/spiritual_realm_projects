## 下一阶段总目标（Phase D + Phase E）

1. 在现有编排架构上，新增 3 类工作流：

   * `write_story`：写小说/续写/角色扮演
   * `teach_explain`：教学讲解 + 练习
   * `rag_qa`：基于本地知识库的问答（RAG）

2. 引入简单但真实可用的 **任务反馈 → Router 学习闭环**：

   * 记录每次任务的指标（质量 + 成本）
   * 用这些日志去 **训练 / 微调 Router**，甚至加一个简单 bandit/RL 层
   * 思路参考 OpenAGI 的 RLTF：用任务结果作为奖励，改进任务规划与模型选择 ([arXiv][1])
   * 参考 Leeroo Orchestrator：用“查询→编排→评估”的循环给 orchestrator 造训练数据，让它学会不跑所有专家就能预测谁最适合当前任务 ([arXiv][2])

---

# Phase D：多任务能力扩展（小说 / 教学 / 本地知识库）

### D1. Workflow 扩展：新增 3 条工作流

在现有 `WorkflowRegistry` 基础上新增：

1. **`write_story` 工作流**

   * 典型输入：

     * 题目 / 背景（如“魔法学院”“科幻太空”）
     * 风格（搞笑 / 严肃 / 童话 / 奇幻…）
     * 角色设定（可选）
     * 大致篇幅（短篇/中篇/章节数）
   * 推荐步骤：

     1. `StoryPlanStep`：根据 question 生成简单大纲（3–7 个情节节点）
     2. `StoryGenerateStep`：按大纲逐段生成正文（可能循环多次）
   * 模型：

     * 复用现在的 `generator` pipeline，增加专用 Prompt 模板
     * 如后期想更好，可以加一个“小说专用生成模型”并在 ModelRegistry 中标记 `capability=["story"]`

2. **`teach_explain` 工作流**

   * 典型输入：

     * 学习主题（如“分数加减法”“印象派绘画简介”）
     * 对象年龄/年级（K-5，与你教师身份也高度契合 😊）
   * 推荐步骤：

     1. `TeachingPlanStep`：生成教学结构（引入→概念→例子→练习→总结）
     2. `TeachingExplainStep`：生成讲解内容
     3. `ExerciseGenerateStep`：生成若干练习题及参考答案
   * 模型：

     * 复用 `generator`，通过不同 Prompt 约束输出结构（分小节、带例题）
   * 后续可以为 `teach_explain` 加一个可选 `web_search` 步骤，用来获取最新教材或案例。

3. **`rag_qa` 工作流（本地知识库问答）**

   * 功能：在 **受限网络 / 完全离线** 状态下，依然能基于本地文档回答问题。
   * 推荐步骤：

     1. `EmbedQueryStep`：用小型 embedding 模型（如 MiniLM）对 question 编码
     2. `RetrieveDocsStep`：在本地向量库中检索 top-k 文档块
     3. `RagAnswerStep`：把检索到的片段 + 问题喂给 `generator` 生成答案
   * RAG 这种 “检索 + 生成” 框架已经是工业标准做法，用于提升 LLM 的事实性和可控性。([阿里云开发者社区][3])

---

### D2. 向量库 & 本地知识库实现细节

新增模块 `mm_orch/tools/vector_store.py`（或单独目录 `kb/`）：

1. 选一个小的 embedding 模型：

   * 推荐：`all-MiniLM-L6-v2` 或其它 MiniLM 家族（性能/速度/显存都很好）([LangChain Blog][4])
   * 通过 Transformers / sentence-transformers 加载

2. 向量库实现：

   * MVP：用 **FAISS** 或简单 numpy + 余弦相似度（数据量不大时足够）
   * 提供 API：

     * `add_documents(docs: List[Doc]) -> ids`
     * `search(query_embedding, top_k) -> List[(doc_id, score)]`

3. 文档导入脚本（方便你后续自己喂教材/PPT/讲义）：

   * `scripts/ingest_docs.py --paths ... --chunk-size 512 --overlap 64`
   * 将 chunk 文本 + embedding 存入一个轻量数据库（如 sqlite + faiss index pickle）

4. `rag_qa` 工作流：

   * 若用户只给 question：走本地 RAG
   * 若用户同时给 question + `--urls`：可以组合 “外部 URL 摘要 + 本地知识库” 两路检索

---

### D3. Router 支持多任务

对现有 Router v2（分类器 + 成本感知），增加新的 label：

* `search_qa_strict`
* `summarize_url`
* `write_story`
* `teach_explain`
* `rag_qa`

数据来源：

* 利用现在的 `WorkflowRegistry` + 规则路由为新任务打初始标签
* 对每个 task type 写 30–100 条示例，手动选合适 workflow，作为 Router 训练初版标签

---

# Phase E：自我优化调度器（从“分类器”走向“反馈驱动的 Router”）

参考 OpenAGI 提出的 RLTF（Reinforcement Learning from Task Feedback）：用任务执行结果作为奖励信号去改进 LLM 控制器的规划能力，构成一个自我改进的闭环 ([arXiv][1])。你现在有 trace + eval，非常适合做一个工程版的小 RLTF。

### E1. 日志中明确写出 “奖励” 字段

在现有 trace JSONL 中补充或规范这些字段：

* `quality_score`：

  * 对 QA / RAG：

    * 有引用 + 调用了正确工具 → +1
    * 内容长度合理（不过短/不过长） → +1
    * 如有基准答案，可用简单字符串相似度得分
  * 对 `write_story`：

    * 字数在区间内 → +1
    * 含角色名、场景描写 → +1
  * 对 `teach_explain`：

    * 是否分段（引入/例子/练习/总结） → +1
* `cost_score`（越小越好，可以是负值）：

  * 延迟（总耗时）
  * 加载次数
  * 显存峰值

最后计算：

* `reward = quality_score - λ * normalized_cost`

把 `reward` 一起写入 JSONL，形成后续学习数据。

### E2. Router 变成 “候选集 + bandit” 模式

目标：Router 不再只输出一个 workflow，而是输出一个 **候选集合 + 初始概率**，然后在运行中根据 reward 调整。

实现建议：

1. Router v2 输出：

   * `candidates = [ (workflow_name, p0), ... ]`
2. 增加一个轻量 bandit 层（如 epsilon-greedy 或 softmax bandit）：

   * 每个 `(task_type, workflow_name)` 作为一个臂
   * 在线更新 Q 值：`Q <- Q + α * (reward - Q)`

这样你就有了：

* 一个基于文本理解的 “静态 Router”（分类器）
* 加上一个逐任务更新的 “经验修正层”（bandit）

这种“Router + bandit”的组合，是很多多专家系统中常见的工程做法（而学术界近年来在 MoE / Mixture-of-Routers 方向也在探索类似思路：多个路由器共同决定专家权重，并通过学习提高整体表现与鲁棒性）([arXiv][5])。

### E3. Offline 训练 Router（利用日志）

写一个 `scripts/train_router_from_logs.py`：

1. 读 trace JSONL
2. 对每个样本：

   * 输入：`question` + 任务特征（如是否包含“最新、上网、写小说、总结、教学”等）
   * 标签：

     * 第一版：使用 “规则 / 现有 Router 选择的 workflow”
     * 升级版：从同一问题多次运行中选 **reward 最高的 workflow** 作为标签（这就开始接近 OpenAGI 的 RLTF 思路：用任务反馈改进模型）([arXiv][1])
3. 训练一个小模型（MiniLM / DistilBERT 分类器）

训练完成后：

* 替换 Router v2 的底层分类器
* 继续保留 bandit 层做在线修正
